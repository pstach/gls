/*
 * AUTOGENERATED tcarstens January 2014
 */
#ifndef __UL96_0__
#define __UL96_0__

#include "../stdint.hl"
#include "../rng.hl"


/*
 * ul96
 */
typedef struct ul96_s {
    uint32_t x[3];
} ul96[1];

inline void ul96_init(ul96 x) { return; }
inline void ul96_clear(ul96 x) { return; }

typedef struct mod96_s {
    ul96 n;
    uint32_t np;
    ul96 rsq;
} mod96[1];




/*
 * Setters
 */
inline void ul96_set_gp(__global ul96 dst, ul96 src) {
    dst->x[0] = src->x[0];
    dst->x[1] = src->x[1];
    dst->x[2] = src->x[2];
}
inline void ul96_set_pg(ul96 dst, __global ul96 src) {
    dst->x[0] = src->x[0];
    dst->x[1] = src->x[1];
    dst->x[2] = src->x[2];
}
inline void ul96_set_gg(__global ul96 dst, __global ul96 src) {
    dst->x[0] = src->x[0];
    dst->x[1] = src->x[1];
    dst->x[2] = src->x[2];
}
inline void ul96_set(ul96 dst, ul96 src) {
    dst->x[0] = src->x[0];
    dst->x[1] = src->x[1];
    dst->x[2] = src->x[2];
}
/*
 * Set a ul96 to a uint32_t
 */
inline void ul96_set_ui(ul96 dst, uint32_t i) {
    dst->x[0] = i;
    dst->x[1] = 0;
    dst->x[2] = 0;
}

/*
 * Get a uint32_t out of a ul96
 */
inline uint32_t ul96_get_ui(ul96 src) {
    return src->x[0];
}




/*
 * Generate a random ul96
 */
inline void ul96_rand(struct rng_t *r, ul96 dst) {
    uint64_t w0 = rand_uint64(r);
    uint64_t w1 = rand_uint64(r);
    
    dst->x[0] = w0 & 0xffffffff;
    dst->x[1] = w0  >> 32;
    dst->x[2] = w1 & 0xffffffff;
}



/*
 * Compare two ul96's
 */
inline int ul96_cmp(ul96 src1, ul96 src2) {
    int r = 0;
    if (src1->x[2] > src2->x[2]) r = 1;
    else if (src1->x[2] < src2->x[2]) r = -1;
    else if (src1->x[1] > src2->x[1]) r = 1;
    else if (src1->x[1] < src2->x[1]) r = -1;
    else if (src1->x[0] > src2->x[0]) r = 1;
    else if (src1->x[0] < src2->x[0]) r = -1;
    return r;
}

/*
 * Compare a ul96 with a uint32_t
 */
inline int ul96_cmp_ui(ul96 src1, uint32_t src2) {
    int r = 0;
    if (src1->x[2] |  src1->x[1]) r = 1;
    else if (src1->x[0] > src2) r = 1;
    else if (src1->x[0] < src2) r = -1;
    return r;
}




/*
 * Add two ul96's
 */
inline void ul96_add(ul96 dst, ul96 src1, ul96 src2) {
    #if defined(UL_NVIDIA)
        asm(
          "add.cc.u32  %0, %3, %6;\n\t"
          "addc.cc.u32 %1, %4, %7;\n\t"
          "addc.u32    %2, %5, %8;\n\t"
          : "=r" (dst->x[0]), "=r" (dst->x[1]), "=r" (dst->x[2])
          : "r" (src1->x[0]), "r" (src1->x[1]), "r" (src1->x[2]), 
            "r" (src2->x[0]), "r" (src2->x[1]), "r" (src2->x[2])
          : "cc"
        );
    #else
        ul96 d = { 0 };
        d->x[0] = (src1->x[0] & 0x7fffffff) + (src2->x[0] & 0x7fffffff) + 0;
        uint32_t c0 = (src1->x[0] >> 31) + (src2->x[0] >> 31) + (d->x[0] >> 31);
        d->x[0] = (c0 << 31) | (d->x[0] & 0x7fffffff);
        c0 = c0 >> 1;
        dst->x[0] = d->x[0];
        
        d->x[1] = (src1->x[1] & 0x7fffffff) + (src2->x[1] & 0x7fffffff) + c0;
        uint32_t c1 = (src1->x[1] >> 31) + (src2->x[1] >> 31) + (d->x[1] >> 31);
        d->x[1] = (c1 << 31) | (d->x[1] & 0x7fffffff);
        c1 = c1 >> 1;
        dst->x[1] = d->x[1];
        
        d->x[2] = (src1->x[2] & 0x7fffffff) + (src2->x[2] & 0x7fffffff) + c1;
        uint32_t c2 = (src1->x[2] >> 31) + (src2->x[2] >> 31) + (d->x[2] >> 31);
        d->x[2] = (c2 << 31) | (d->x[2] & 0x7fffffff);
        c2 = c2 >> 1;
        dst->x[2] = d->x[2];
        
    #endif
    return;
}
/*
 * Sub two ul96's
 */
inline void ul96_sub(ul96 dst, ul96 src1, ul96 src2) {
    #if defined(UL_NVIDIA)
        asm(
          "sub.cc.u32  %0, %3, %6;\n\t"
          "subc.cc.u32 %1, %4, %7;\n\t"
          "subc.u32    %2, %5, %8;\n\t"
          : "=r" (dst->x[0]), "=r" (dst->x[1]), "=r" (dst->x[2])
          : "r" (src1->x[0]), "r" (src1->x[1]), "r" (src1->x[2]), 
            "r" (src2->x[0]), "r" (src2->x[1]), "r" (src2->x[2])
          : "cc"
        );
    #else
        ul96 d = { 0 };
        d->x[0] = (src1->x[0] & 0x7fffffff) - (src2->x[0] & 0x7fffffff) - 0;
        uint32_t b0 = (src1->x[0] >> 31) - (src2->x[0] >> 31) - (d->x[0] >> 31);
        d->x[0] = (b0 << 31) | (d->x[0] & 0x7fffffff);
        b0 = (b0 >> 1) & 1;
        dst->x[0] = d->x[0];
        
        d->x[1] = (src1->x[1] & 0x7fffffff) - (src2->x[1] & 0x7fffffff) - b0;
        uint32_t b1 = (src1->x[1] >> 31) - (src2->x[1] >> 31) - (d->x[1] >> 31);
        d->x[1] = (b1 << 31) | (d->x[1] & 0x7fffffff);
        b1 = (b1 >> 1) & 1;
        dst->x[1] = d->x[1];
        
        d->x[2] = (src1->x[2] & 0x7fffffff) - (src2->x[2] & 0x7fffffff) - b1;
        uint32_t b2 = (src1->x[2] >> 31) - (src2->x[2] >> 31) - (d->x[2] >> 31);
        d->x[2] = (b2 << 31) | (d->x[2] & 0x7fffffff);
        b2 = (b2 >> 1) & 1;
        dst->x[2] = d->x[2];
        
    #endif
    return;
}
/*
 * Mul two ul96's
 */
inline void ul96_mul(ul96 dst, ul96 src1, ul96 src2) {
    #if defined(UL_NVIDIA)
        asm(
          "mul.lo.u32 %0, %3, %6;\n\t"
          "mul.hi.u32 %1, %3, %6;\n\t"
          "mad.lo.cc.u32 %1, %3, %7, %1;\n\t"
          "madc.hi.u32 %2, %3, %7, 0;\n\t"
          "mad.lo.cc.u32 %1, %4, %6, %1;\n\t"
          "madc.hi.u32 %2, %4, %6, %2;\n\t"
          "mad.lo.u32 %2, %3, %8, %2;\n\t"
          "mad.lo.u32 %2, %4, %7, %2;\n\t"
          "mad.lo.u32 %2, %5, %6, %2;\n\t"
          : "=r" (dst->x[0]), "=r" (dst->x[1]), "=r" (dst->x[2])
          : "r" (src1->x[0]), "r" (src1->x[1]), "r" (src1->x[2]), 
            "r" (src2->x[0]), "r" (src2->x[1]), "r" (src2->x[2])
          : "cc"
        );
    #else
        ul96 d = {0};
        
        uint64_t tmp0 = ((uint64_t)src1->x[0]) * ((uint64_t)src2->x[0]);
        ul96 tmp0_;
        tmp0_->x[0] = tmp0;
        tmp0_->x[1] = tmp0 >> 32;
        tmp0_->x[2] = 0;
        ul96_add(d, tmp0_, d);
        
        uint64_t tmp1 = ((uint64_t)src1->x[0]) * ((uint64_t)src2->x[1]);
        ul96 tmp1_;
        tmp1_->x[0] = 0;
        tmp1_->x[1] = tmp1;
        tmp1_->x[2] = tmp1 >> 32;
        ul96_add(d, tmp1_, d);
        
        uint64_t tmp2 = ((uint64_t)src1->x[1]) * ((uint64_t)src2->x[0]);
        ul96 tmp2_;
        tmp2_->x[0] = 0;
        tmp2_->x[1] = tmp2;
        tmp2_->x[2] = tmp2 >> 32;
        ul96_add(d, tmp2_, d);
        
        uint64_t tmp3 = ((uint64_t)src1->x[0]) * ((uint64_t)src2->x[2]);
        ul96 tmp3_;
        tmp3_->x[0] = 0;
        tmp3_->x[1] = 0;
        tmp3_->x[2] = tmp3;
        ul96_add(d, tmp3_, d);
        
        uint64_t tmp4 = ((uint64_t)src1->x[1]) * ((uint64_t)src2->x[1]);
        ul96 tmp4_;
        tmp4_->x[0] = 0;
        tmp4_->x[1] = 0;
        tmp4_->x[2] = tmp4;
        ul96_add(d, tmp4_, d);
        
        uint64_t tmp5 = ((uint64_t)src1->x[2]) * ((uint64_t)src2->x[0]);
        ul96 tmp5_;
        tmp5_->x[0] = 0;
        tmp5_->x[1] = 0;
        tmp5_->x[2] = tmp5;
        ul96_add(d, tmp5_, d);
        
        dst->x[0] = d->x[0];
        dst->x[1] = d->x[1];
        dst->x[2] = d->x[2];
    #endif
    return;
}




/*
 * Initialize mod96
 */
inline void mod96_init(mod96 n) {
}

/*
 * Add two ul96's modulo another
 */
inline void ul96_modadd(ul96 dst, ul96 src1, ul96 src2, mod96 n) {
    ul96_add(dst, src1, src2);
    if (ul96_cmp(dst, n->n) >= 0)
        ul96_sub(dst, dst, n->n);
}

/*
 * Subtract one ul96 from another modulo a third
 */
inline void ul96_modsub(ul96 dst, ul96 src1, ul96 src2, mod96 n) {
    ul96 tr1, tr2;
    ul96_sub(tr1, src1, src2);
    ul96_add(tr2, tr1, n->n);
    if (ul96_cmp(src1, src2) >= 0)
        ul96_set(dst, tr1);
    else
        ul96_set(dst, tr2);
}

/*
 * Mul two ul96's modulo a third, followed by Montgomery reduction
 */
void ul96_modmul(ul96 _dst, ul96 _src1, ul96 _src2, mod96 n) {
    #if defined(UL_NVIDIA)
        volatile ul96 src1;
        volatile ul96 src2;
        /* ul96_set(src1, _src1); */
        src1->x[0] = _src1->x[0];
        src1->x[1] = _src1->x[1];
        src1->x[2] = _src1->x[2];
        /* ul96_set(src2, _src2); */
        src2->x[0] = _src2->x[0];
        src2->x[1] = _src2->x[1];
        src2->x[2] = _src2->x[2];
        
        uint32_t q = 0;
        ul96 dst = { 0 };
        uint32_t dst_3 = 0;
        
        asm(
          /* Compute c_0..2 for the product a*b, with carry-out to dst_3 */
          "mad.lo.cc.u32  %0, %5, %8, %0;\n\t"  /* c_0 += lo(a_0, b_0) */
          "madc.hi.cc.u32 %1, %5, %8, %1;\n\t"  /* c_1 += hi(a_0, b_0) */
          "madc.lo.cc.u32 %2, %5, %10, %2;\n\t"  /* c_2 += lo(a_0, b_2) */
          "addc.u32 %3, %3, 0;\n\t"  /* accum carry in c_3 */
          "mad.lo.cc.u32  %1, %5, %9, %1;\n\t"  /* c_1 += lo(a_0, b_1) */
          "madc.hi.cc.u32 %2, %5, %9, %2;\n\t"  /* c_2 += hi(a_0, b_1) */
          "addc.u32 %3, %3, 0;\n\t"  /* accum carry in c_3 */
          "mad.lo.cc.u32  %1, %6, %8, %1;\n\t"  /* c_1 += lo(a_1, b_0) */
          "madc.hi.cc.u32 %2, %6, %8, %2;\n\t"  /* c_2 += hi(a_1, b_0) */
          "addc.u32 %3, %3, 0;\n\t"  /* accum carry in c_3 */
          "mad.lo.cc.u32  %2, %6, %9, %2;\n\t"  /* c_2 += lo(a_1, b_1) */
          "addc.u32 %3, %3, 0;\n\t"  /* accum carry in c_3 */
          "mad.lo.cc.u32  %2, %7, %8, %2;\n\t"  /* c_2 += lo(a_2, b_0) */
          "addc.u32 %3, %3, 0;\n\t"  /* accum carry in c_3 */
          
          /* Add in the qN's */
          /* n = 0... */
            /* Compute q = mu * c_0 */
            "mov.u32 %4, %0;\n\t"
            "mul.lo.u32 %4, %4, %14;\n\t"
            /* Update c_0 with qN_0 */
            "mad.lo.cc.u32 %0, %4, %11, %0;\n\t"  /* c_0 += lo(q, n_0) */
            /* Shift */
            "mov.u32 %0, %1;\n\t"  /* dst_0 <- c_1 */
            "mov.u32 %1, %2;\n\t"  /* dst_1 <- c_2 */
            "mov.u32 %2, %3;\n\t"  /* dst_2 <- c_3 */
            "xor.b32 %3, %3, %3;\n\t"  /* dst_3 <- c_4 */
            /* Compute and add-in qN, with carry-out to dst_3 */
            "madc.hi.cc.u32 %0, %4, %11, %0;\n\t"  /* c_1 += hi(q, n_0) */
            "madc.lo.cc.u32 %1, %4, %13, %1;\n\t"  /* c_2 += lo(q, n_2) */
            "madc.hi.cc.u32 %2, %4, %13, %2;\n\t"  /* c_3 += hi(q, n_2) */
            "addc.u32    %3, %3, 0;\n\t"  /* accum carry in dst_3 */
            "mad.lo.cc.u32  %0, %4, %12, %0;\n\t"  /* c_1 += lo(q, n_1) */
            "madc.hi.cc.u32 %1, %4, %12, %1;\n\t"  /* c_2 += hi(q, n_1) */
            "addc.cc.u32 %2, %2, 0;\n\t"  /* accum carry in dst_2 */
            "addc.u32    %3, %3, 0;\n\t"  /* accum carry in dst_3 */
          /* n = 1... */
            /* Compute q = mu * c_1 */
            "mov.u32 %4, %0;\n\t"
            "mul.lo.u32 %4, %4, %14;\n\t"
            /* Update c_1 with qN_0 */
            "mad.lo.cc.u32 %0, %4, %11, %0;\n\t"  /* c_1 += lo(q, n_0) */
            /* Shift */
            "mov.u32 %0, %1;\n\t"  /* dst_0 <- c_2 */
            "mov.u32 %1, %2;\n\t"  /* dst_1 <- c_3 */
            "mov.u32 %2, %3;\n\t"  /* dst_2 <- c_4 */
            "xor.b32 %3, %3, %3;\n\t"  /* dst_3 <- c_5 */
            /* Compute and add-in qN, with carry-out to dst_3 */
            "madc.hi.cc.u32 %0, %4, %11, %0;\n\t"  /* c_2 += hi(q, n_0) */
            "madc.lo.cc.u32 %1, %4, %13, %1;\n\t"  /* c_3 += lo(q, n_2) */
            "madc.hi.cc.u32 %2, %4, %13, %2;\n\t"  /* c_4 += hi(q, n_2) */
            "addc.u32    %3, %3, 0;\n\t"  /* accum carry in dst_3 */
            "mad.lo.cc.u32  %0, %4, %12, %0;\n\t"  /* c_1 += lo(q, n_1) */
            "madc.hi.cc.u32 %1, %4, %12, %1;\n\t"  /* c_2 += hi(q, n_1) */
            "addc.cc.u32 %2, %2, 0;\n\t"  /* accum carry in dst_2 */
            "addc.u32    %3, %3, 0;\n\t"  /* accum carry in dst_3 */
          /* n = 2... */
            /* Compute q = mu * c_2 */
            "mov.u32 %4, %0;\n\t"
            "mul.lo.u32 %4, %4, %14;\n\t"
            /* Update c_2 with qN_0 */
            "mad.lo.cc.u32 %0, %4, %11, %0;\n\t"  /* c_2 += lo(q, n_0) */
            /* Shift */
            "mov.u32 %0, %1;\n\t"  /* dst_0 <- c_3 */
            "mov.u32 %1, %2;\n\t"  /* dst_1 <- c_4 */
            "mov.u32 %2, %3;\n\t"  /* dst_2 <- c_5 */
            "xor.b32 %3, %3, %3;\n\t"  /* dst_3 <- c_6 */
            /* Compute and add-in qN, with carry-out to dst_3 */
            "madc.hi.cc.u32 %0, %4, %11, %0;\n\t"  /* c_3 += hi(q, n_0) */
            "madc.lo.cc.u32 %1, %4, %13, %1;\n\t"  /* c_4 += lo(q, n_2) */
            "madc.hi.cc.u32 %2, %4, %13, %2;\n\t"  /* c_5 += hi(q, n_2) */
            "addc.u32    %3, %3, 0;\n\t"  /* accum carry in dst_3 */
            "mad.lo.cc.u32  %0, %4, %12, %0;\n\t"  /* c_1 += lo(q, n_1) */
            "madc.hi.cc.u32 %1, %4, %12, %1;\n\t"  /* c_2 += hi(q, n_1) */
            "addc.cc.u32 %2, %2, 0;\n\t"  /* accum carry in dst_2 */
            "addc.u32    %3, %3, 0;\n\t"  /* accum carry in dst_3 */
          
          /* Compute c_3..5 in the product a*b, storing the result in dst_0..2, with carry-out to dst_3 */
          "mad.hi.cc.u32  %0, %5, %10, %0;\n\t"  /* c_3 += hi(a_0, b_2) */
          "addc.cc.u32 %1, %1, 0;\n\t"  /* accum carry in c_4 */
          "addc.cc.u32 %2, %2, 0;\n\t"  /* accum carry in c_5 */
          "addc.u32    %3, %3, 0;\n\t"  /* accum carry in c_6 */
          "mad.lo.cc.u32  %0, %6, %10, %0;\n\t"  /* c_3 += lo(a_1, b_2) */
          "madc.hi.cc.u32 %1, %6, %10, %1;\n\t"  /* c_4 += hi(a_1, b_2) */
          "addc.cc.u32 %2, %2, 0;\n\t"  /* accum carry in c_5 */
          "addc.u32    %3, %3, 0;\n\t"  /* accum carry in c_6 */
          "mad.hi.cc.u32  %0, %6, %9, %0;\n\t"  /* c_3 += hi(a_1, b_1) */
          "addc.cc.u32 %1, %1, 0;\n\t"  /* accum carry in c_4 */
          "addc.cc.u32 %2, %2, 0;\n\t"  /* accum carry in c_5 */
          "addc.u32    %3, %3, 0;\n\t"  /* accum carry in c_6 */
          "mad.hi.cc.u32  %0, %7, %8, %0;\n\t"  /* c_3 += hi(a_2, b_0) */
          "madc.lo.cc.u32 %1, %7, %10, %1;\n\t"  /* c_4 += lo(a_2, b_2) */
          "madc.hi.cc.u32 %2, %7, %10, %2;\n\t"  /* c_5 += hi(a_2, b_2) */
          "addc.u32    %3, %3, 0;\n\t"  /* accum carry in c_6 */
          "mad.lo.cc.u32  %0, %7, %9, %0;\n\t"  /* c_3 += lo(a_2, b_1) */
          "madc.hi.cc.u32 %1, %7, %9, %1;\n\t"  /* c_4 += hi(a_2, b_1) */
          "addc.cc.u32 %2, %2, 0;\n\t"  /* accum carry in c_5 */
          "addc.u32    %3, %3, 0;\n\t"  /* accum carry in c_6 */
          
          : "+r" (dst->x[0]), "+r" (dst->x[1]), "+r" (dst->x[2]), "+r" (dst_3), "+r" (q)
          : "r" (src1->x[0]), "r" (src1->x[1]), "r" (src1->x[2]), 
            "r" (src2->x[0]), "r" (src2->x[1]), "r" (src2->x[2]), 
            "r" (n->n->x[0]), "r" (n->n->x[1]), "r" (n->n->x[2]), 
            "r" (n->np)
        );
        
        /* Reduce as needed */
        if (dst_3 || (ul96_cmp(dst, n->n) >= 0))
            ul96_sub(dst, dst, n->n);
        ul96_set(_dst, dst);
    #else
        /* The pairwise products of the a_i and b_j */
        const uint64_t a_0__b_0 = ((uint64_t)_src1->x[0]) * ((uint64_t)_src2->x[0]);
        const uint32_t a_0__b_0_lo = a_0__b_0;
        const uint32_t a_0__b_0_hi = a_0__b_0 >> 32;
        const uint64_t a_0__b_1 = ((uint64_t)_src1->x[0]) * ((uint64_t)_src2->x[1]);
        const uint32_t a_0__b_1_lo = a_0__b_1;
        const uint32_t a_0__b_1_hi = a_0__b_1 >> 32;
        const uint64_t a_0__b_2 = ((uint64_t)_src1->x[0]) * ((uint64_t)_src2->x[2]);
        const uint32_t a_0__b_2_lo = a_0__b_2;
        const uint32_t a_0__b_2_hi = a_0__b_2 >> 32;
        const uint64_t a_1__b_0 = ((uint64_t)_src1->x[1]) * ((uint64_t)_src2->x[0]);
        const uint32_t a_1__b_0_lo = a_1__b_0;
        const uint32_t a_1__b_0_hi = a_1__b_0 >> 32;
        const uint64_t a_1__b_1 = ((uint64_t)_src1->x[1]) * ((uint64_t)_src2->x[1]);
        const uint32_t a_1__b_1_lo = a_1__b_1;
        const uint32_t a_1__b_1_hi = a_1__b_1 >> 32;
        const uint64_t a_1__b_2 = ((uint64_t)_src1->x[1]) * ((uint64_t)_src2->x[2]);
        const uint32_t a_1__b_2_lo = a_1__b_2;
        const uint32_t a_1__b_2_hi = a_1__b_2 >> 32;
        const uint64_t a_2__b_0 = ((uint64_t)_src1->x[2]) * ((uint64_t)_src2->x[0]);
        const uint32_t a_2__b_0_lo = a_2__b_0;
        const uint32_t a_2__b_0_hi = a_2__b_0 >> 32;
        const uint64_t a_2__b_1 = ((uint64_t)_src1->x[2]) * ((uint64_t)_src2->x[1]);
        const uint32_t a_2__b_1_lo = a_2__b_1;
        const uint32_t a_2__b_1_hi = a_2__b_1 >> 32;
        const uint64_t a_2__b_2 = ((uint64_t)_src1->x[2]) * ((uint64_t)_src2->x[2]);
        const uint32_t a_2__b_2_lo = a_2__b_2;
        const uint32_t a_2__b_2_hi = a_2__b_2 >> 32;
        
        /* Limbs of the product C = A*B */
        uint32_t c_0 = 0;
        uint32_t q_0 = 0;
        uint32_t c_1 = 0;
        uint32_t q_1 = 0;
        uint32_t c_2 = 0;
        uint32_t q_2 = 0;
        uint32_t c_3 = 0;
        uint32_t q_3 = 0;
        uint32_t c_4 = 0;
        uint32_t q_4 = 0;
        uint32_t c_5 = 0;
        uint32_t q_5 = 0;
        uint32_t c_6 = 0;
        uint32_t q_6 = 0;
        
        /* The product of the q_i's with N */
        uint64_t q_0__N_0 = 0;
        uint64_t q_0__N_1 = 0;
        uint64_t q_0__N_2 = 0;
        uint64_t q_1__N_0 = 0;
        uint64_t q_1__N_1 = 0;
        uint64_t q_1__N_2 = 0;
        uint64_t q_2__N_0 = 0;
        uint64_t q_2__N_1 = 0;
        uint64_t q_2__N_2 = 0;
        
        /* Compute c_0 */
        {
            /* Add the product terms into c_0, accumulating carries in c_1 */
            c_1 += ul32_addc(&c_0, &c_0, &a_0__b_0_lo);
            
            /* Add the q_i*N's for i < 0 */
            
            /* Compute q_0 and add its product with N */
            q_0 = n->np * c_0;
            q_0__N_0 = ((uint64_t)q_0) * ((uint64_t)n->n->x[0]);
            q_0__N_1 = ((uint64_t)q_0) * ((uint64_t)n->n->x[1]);
            q_0__N_2 = ((uint64_t)q_0) * ((uint64_t)n->n->x[2]);
            const uint32_t q_0__N_0_lo = q_0__N_0;
            c_1 += ul32_addc(&c_0, &c_0, &q_0__N_0_lo);
        }
        
        /* Compute c_1 */
        {
            /* Add the product terms into c_1, accumulating carries in c_2 */
            c_2 += ul32_addc(&c_1, &c_1, &a_0__b_0_hi);
            c_2 += ul32_addc(&c_1, &c_1, &a_0__b_1_lo);
            c_2 += ul32_addc(&c_1, &c_1, &a_1__b_0_lo);
            
            /* Add the q_i*N's for i < 1 */
            const uint32_t q_0__N_1_lo = q_0__N_1;
            c_2 += ul32_addc(&c_1, &c_1, &q_0__N_1_lo);
            const uint32_t q_0__N_0_hi = q_0__N_0 >> 32;
            c_2 += ul32_addc(&c_1, &c_1, &q_0__N_0_hi);
            
            /* Compute q_1 and add its product with N */
            q_1 = n->np * c_1;
            q_1__N_0 = ((uint64_t)q_1) * ((uint64_t)n->n->x[0]);
            q_1__N_1 = ((uint64_t)q_1) * ((uint64_t)n->n->x[1]);
            q_1__N_2 = ((uint64_t)q_1) * ((uint64_t)n->n->x[2]);
            const uint32_t q_1__N_0_lo = q_1__N_0;
            c_2 += ul32_addc(&c_1, &c_1, &q_1__N_0_lo);
        }
        
        /* Compute c_2 */
        {
            /* Add the product terms into c_2, accumulating carries in c_3 */
            c_3 += ul32_addc(&c_2, &c_2, &a_0__b_1_hi);
            c_3 += ul32_addc(&c_2, &c_2, &a_0__b_2_lo);
            c_3 += ul32_addc(&c_2, &c_2, &a_1__b_0_hi);
            c_3 += ul32_addc(&c_2, &c_2, &a_1__b_1_lo);
            c_3 += ul32_addc(&c_2, &c_2, &a_2__b_0_lo);
            
            /* Add the q_i*N's for i < 2 */
            const uint32_t q_0__N_2_lo = q_0__N_2;
            c_3 += ul32_addc(&c_2, &c_2, &q_0__N_2_lo);
            const uint32_t q_0__N_1_hi = q_0__N_1 >> 32;
            c_3 += ul32_addc(&c_2, &c_2, &q_0__N_1_hi);
            const uint32_t q_1__N_1_lo = q_1__N_1;
            c_3 += ul32_addc(&c_2, &c_2, &q_1__N_1_lo);
            const uint32_t q_1__N_0_hi = q_1__N_0 >> 32;
            c_3 += ul32_addc(&c_2, &c_2, &q_1__N_0_hi);
            
            /* Compute q_2 and add its product with N */
            q_2 = n->np * c_2;
            q_2__N_0 = ((uint64_t)q_2) * ((uint64_t)n->n->x[0]);
            q_2__N_1 = ((uint64_t)q_2) * ((uint64_t)n->n->x[1]);
            q_2__N_2 = ((uint64_t)q_2) * ((uint64_t)n->n->x[2]);
            const uint32_t q_2__N_0_lo = q_2__N_0;
            c_3 += ul32_addc(&c_2, &c_2, &q_2__N_0_lo);
        }
        
        /* Compute c_3 */
        {
            /* Add the product terms into c_3, accumulating carries in c_4 */
            c_4 += ul32_addc(&c_3, &c_3, &a_0__b_2_hi);
            c_4 += ul32_addc(&c_3, &c_3, &a_1__b_1_hi);
            c_4 += ul32_addc(&c_3, &c_3, &a_1__b_2_lo);
            c_4 += ul32_addc(&c_3, &c_3, &a_2__b_0_hi);
            c_4 += ul32_addc(&c_3, &c_3, &a_2__b_1_lo);
            
            /* Add the q_i*N's for i < 3 */
            const uint32_t q_0__N_2_hi = q_0__N_2 >> 32;
            c_4 += ul32_addc(&c_3, &c_3, &q_0__N_2_hi);
            const uint32_t q_1__N_2_lo = q_1__N_2;
            c_4 += ul32_addc(&c_3, &c_3, &q_1__N_2_lo);
            const uint32_t q_1__N_1_hi = q_1__N_1 >> 32;
            c_4 += ul32_addc(&c_3, &c_3, &q_1__N_1_hi);
            const uint32_t q_2__N_1_lo = q_2__N_1;
            c_4 += ul32_addc(&c_3, &c_3, &q_2__N_1_lo);
            const uint32_t q_2__N_0_hi = q_2__N_0 >> 32;
            c_4 += ul32_addc(&c_3, &c_3, &q_2__N_0_hi);
            
        }
        
        /* Compute c_4 */
        {
            /* Add the product terms into c_4, accumulating carries in c_5 */
            c_5 += ul32_addc(&c_4, &c_4, &a_1__b_2_hi);
            c_5 += ul32_addc(&c_4, &c_4, &a_2__b_1_hi);
            c_5 += ul32_addc(&c_4, &c_4, &a_2__b_2_lo);
            
            /* Add the q_i*N's for i < 4 */
            const uint32_t q_1__N_2_hi = q_1__N_2 >> 32;
            c_5 += ul32_addc(&c_4, &c_4, &q_1__N_2_hi);
            const uint32_t q_2__N_2_lo = q_2__N_2;
            c_5 += ul32_addc(&c_4, &c_4, &q_2__N_2_lo);
            const uint32_t q_2__N_1_hi = q_2__N_1 >> 32;
            c_5 += ul32_addc(&c_4, &c_4, &q_2__N_1_hi);
            
        }
        
        /* Compute c_5 */
        {
            /* Add the product terms into c_5, accumulating carries in c_6 */
            c_6 += ul32_addc(&c_5, &c_5, &a_2__b_2_hi);
            
            /* Add the q_i*N's for i < 5 */
            const uint32_t q_2__N_2_hi = q_2__N_2 >> 32;
            c_6 += ul32_addc(&c_5, &c_5, &q_2__N_2_hi);
            
        }
        
        /* R = C * beta^{-n} */
        _dst->x[0] = c_3;
        _dst->x[1] = c_4;
        _dst->x[2] = c_5;
        
        /* Reduce as needed */
        if (c_6 || (ul96_cmp(_dst, n->n) >= 0))
            ul96_sub(_dst, _dst, n->n);
    #endif
}

/*
 * Convert a ul96 into Montgomery form
 */
inline void ul96_to_montgomery(ul96 dst, ul96 src, mod96 mod) {
    ul96_modmul(dst, src, mod->rsq, mod);
}

/*
 * Convert a ul96 out-of Montgomery form
 */
inline void ul96_from_montgomery(ul96 dst, ul96 src, mod96 mod) {
    ul96 one = { 0 };
    one->x[0] = 1;
    
    ul96_modmul(dst, src, one, mod);
}




/*
 * Right-shift a ul96 by some number of bits
 */
inline void ul96_rshift(ul96 dst, ul96 src, int shift) {
dst->x[0] = (src->x[0] >> shift) | (src->x[1] << (32 - shift));
dst->x[1] = (src->x[1] >> shift) | (src->x[2] << (32 - shift));
dst->x[2] = dst->x[2] >> shift;
}



/*
 * Left shift a ul96 by some number of words
 */
inline void ul96_lshiftw(ul96 dst, ul96 src, int w) {
    dst->x[2] = ((2-w) >= 0) ? src->x[2-w] : 0;
    dst->x[1] = ((1-w) >= 0) ? src->x[1-w] : 0;
    dst->x[0] = ((0-w) >= 0) ? src->x[0-w] : 0;
}

/*
 * Multiply a ul96 by a uint32_t
 */
inline void ul96_mulu32(ul96 dst, ul96 src, uint32_t x) {
    uint64_t x_src_0 = ((uint64_t)src->x[0]) * ((uint64_t)x);
    uint64_t x_src_1 = ((uint64_t)src->x[1]) * ((uint64_t)x);
    uint64_t x_src_2 = ((uint64_t)src->x[2]) * ((uint64_t)x);
    
    dst->x[0] = 0;
    dst->x[1] = 0;
    dst->x[2] = 0;
    
    *(uint64_t*)(&dst->x[0]) += x_src_0;
    *(uint64_t*)(&dst->x[1]) += x_src_1;
    dst->x[2] += x_src_2;
}




#endif

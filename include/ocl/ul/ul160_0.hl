/*
 * AUTOGENERATED tcarstens January 2014
 */
#ifndef __UL160_0__
#define __UL160_0__

#include "../stdint.hl"
#include "../rng.hl"


/*
 * ul160
 */
typedef struct ul160_s {
    uint32_t x[5];
} ul160[1];

inline void ul160_init(ul160 x) { return; }
inline void ul160_clear(ul160 x) { return; }

typedef struct mod160_s {
    ul160 n;
    uint32_t np;
    ul160 rsq;
} mod160[1];




/*
 * Setters
 */
inline void ul160_set_gp(__global ul160 dst, ul160 src) {
    dst->x[0] = src->x[0];
    dst->x[1] = src->x[1];
    dst->x[2] = src->x[2];
    dst->x[3] = src->x[3];
    dst->x[4] = src->x[4];
}
inline void ul160_set_pg(ul160 dst, __global ul160 src) {
    dst->x[0] = src->x[0];
    dst->x[1] = src->x[1];
    dst->x[2] = src->x[2];
    dst->x[3] = src->x[3];
    dst->x[4] = src->x[4];
}
inline void ul160_set_gg(__global ul160 dst, __global ul160 src) {
    dst->x[0] = src->x[0];
    dst->x[1] = src->x[1];
    dst->x[2] = src->x[2];
    dst->x[3] = src->x[3];
    dst->x[4] = src->x[4];
}
inline void ul160_set(ul160 dst, ul160 src) {
    dst->x[0] = src->x[0];
    dst->x[1] = src->x[1];
    dst->x[2] = src->x[2];
    dst->x[3] = src->x[3];
    dst->x[4] = src->x[4];
}
/*
 * Set a ul160 to a uint32_t
 */
inline void ul160_set_ui(ul160 dst, uint32_t i) {
    dst->x[0] = i;
    dst->x[1] = 0;
    dst->x[2] = 0;
    dst->x[3] = 0;
    dst->x[4] = 0;
}

/*
 * Get a uint32_t out of a ul160
 */
inline uint32_t ul160_get_ui(ul160 src) {
    return src->x[0];
}




/*
 * Generate a random ul160
 */
inline void ul160_rand(struct rng_t *r, ul160 dst) {
    uint64_t w0 = rand_uint64(r);
    uint64_t w1 = rand_uint64(r);
    uint64_t w2 = rand_uint64(r);
    
    dst->x[0] = w0 & 0xffffffff;
    dst->x[1] = w0  >> 32;
    dst->x[2] = w1 & 0xffffffff;
    dst->x[3] = w1  >> 32;
    dst->x[4] = w2 & 0xffffffff;
}



/*
 * Compare two ul160's
 */
inline int ul160_cmp(ul160 src1, ul160 src2) {
    int r = 0;
    if (src1->x[4] > src2->x[4]) r = 1;
    else if (src1->x[4] < src2->x[4]) r = -1;
    else if (src1->x[3] > src2->x[3]) r = 1;
    else if (src1->x[3] < src2->x[3]) r = -1;
    else if (src1->x[2] > src2->x[2]) r = 1;
    else if (src1->x[2] < src2->x[2]) r = -1;
    else if (src1->x[1] > src2->x[1]) r = 1;
    else if (src1->x[1] < src2->x[1]) r = -1;
    else if (src1->x[0] > src2->x[0]) r = 1;
    else if (src1->x[0] < src2->x[0]) r = -1;
    return r;
}

/*
 * Compare a ul160 with a uint32_t
 */
inline int ul160_cmp_ui(ul160 src1, uint32_t src2) {
    int r = 0;
    if (src1->x[4] | src1->x[3] | src1->x[2] |  src1->x[1]) r = 1;
    else if (src1->x[0] > src2) r = 1;
    else if (src1->x[0] < src2) r = -1;
    return r;
}




/*
 * Add two ul160's
 */
inline void ul160_add(ul160 dst, ul160 src1, ul160 src2) {
    #if defined(UL_NVIDIA)
        asm(
          "add.cc.u32  %0, %5, %10;\n\t"
          "addc.cc.u32 %1, %6, %11;\n\t"
          "addc.cc.u32 %2, %7, %12;\n\t"
          "addc.cc.u32 %3, %8, %13;\n\t"
          "addc.u32    %4, %9, %14;\n\t"
          : "=r" (dst->x[0]), "=r" (dst->x[1]), "=r" (dst->x[2]), "=r" (dst->x[3]), "=r" (dst->x[4])
          : "r" (src1->x[0]), "r" (src1->x[1]), "r" (src1->x[2]), "r" (src1->x[3]), "r" (src1->x[4]), 
            "r" (src2->x[0]), "r" (src2->x[1]), "r" (src2->x[2]), "r" (src2->x[3]), "r" (src2->x[4])
          : "cc"
        );
    #else
        ul160 d = { 0 };
        d->x[0] = (src1->x[0] & 0x7fffffff) + (src2->x[0] & 0x7fffffff) + 0;
        uint32_t c0 = (src1->x[0] >> 31) + (src2->x[0] >> 31) + (d->x[0] >> 31);
        d->x[0] = (c0 << 31) | (d->x[0] & 0x7fffffff);
        c0 = c0 >> 1;
        dst->x[0] = d->x[0];
        
        d->x[1] = (src1->x[1] & 0x7fffffff) + (src2->x[1] & 0x7fffffff) + c0;
        uint32_t c1 = (src1->x[1] >> 31) + (src2->x[1] >> 31) + (d->x[1] >> 31);
        d->x[1] = (c1 << 31) | (d->x[1] & 0x7fffffff);
        c1 = c1 >> 1;
        dst->x[1] = d->x[1];
        
        d->x[2] = (src1->x[2] & 0x7fffffff) + (src2->x[2] & 0x7fffffff) + c1;
        uint32_t c2 = (src1->x[2] >> 31) + (src2->x[2] >> 31) + (d->x[2] >> 31);
        d->x[2] = (c2 << 31) | (d->x[2] & 0x7fffffff);
        c2 = c2 >> 1;
        dst->x[2] = d->x[2];
        
        d->x[3] = (src1->x[3] & 0x7fffffff) + (src2->x[3] & 0x7fffffff) + c2;
        uint32_t c3 = (src1->x[3] >> 31) + (src2->x[3] >> 31) + (d->x[3] >> 31);
        d->x[3] = (c3 << 31) | (d->x[3] & 0x7fffffff);
        c3 = c3 >> 1;
        dst->x[3] = d->x[3];
        
        d->x[4] = (src1->x[4] & 0x7fffffff) + (src2->x[4] & 0x7fffffff) + c3;
        uint32_t c4 = (src1->x[4] >> 31) + (src2->x[4] >> 31) + (d->x[4] >> 31);
        d->x[4] = (c4 << 31) | (d->x[4] & 0x7fffffff);
        c4 = c4 >> 1;
        dst->x[4] = d->x[4];
        
    #endif
    return;
}
/*
 * Sub two ul160's
 */
inline void ul160_sub(ul160 dst, ul160 src1, ul160 src2) {
    #if defined(UL_NVIDIA)
        asm(
          "sub.cc.u32  %0, %5, %10;\n\t"
          "subc.cc.u32 %1, %6, %11;\n\t"
          "subc.cc.u32 %2, %7, %12;\n\t"
          "subc.cc.u32 %3, %8, %13;\n\t"
          "subc.u32    %4, %9, %14;\n\t"
          : "=r" (dst->x[0]), "=r" (dst->x[1]), "=r" (dst->x[2]), "=r" (dst->x[3]), "=r" (dst->x[4])
          : "r" (src1->x[0]), "r" (src1->x[1]), "r" (src1->x[2]), "r" (src1->x[3]), "r" (src1->x[4]), 
            "r" (src2->x[0]), "r" (src2->x[1]), "r" (src2->x[2]), "r" (src2->x[3]), "r" (src2->x[4])
          : "cc"
        );
    #else
        ul160 d = { 0 };
        d->x[0] = (src1->x[0] & 0x7fffffff) - (src2->x[0] & 0x7fffffff) - 0;
        uint32_t b0 = (src1->x[0] >> 31) - (src2->x[0] >> 31) - (d->x[0] >> 31);
        d->x[0] = (b0 << 31) | (d->x[0] & 0x7fffffff);
        b0 = (b0 >> 1) & 1;
        dst->x[0] = d->x[0];
        
        d->x[1] = (src1->x[1] & 0x7fffffff) - (src2->x[1] & 0x7fffffff) - b0;
        uint32_t b1 = (src1->x[1] >> 31) - (src2->x[1] >> 31) - (d->x[1] >> 31);
        d->x[1] = (b1 << 31) | (d->x[1] & 0x7fffffff);
        b1 = (b1 >> 1) & 1;
        dst->x[1] = d->x[1];
        
        d->x[2] = (src1->x[2] & 0x7fffffff) - (src2->x[2] & 0x7fffffff) - b1;
        uint32_t b2 = (src1->x[2] >> 31) - (src2->x[2] >> 31) - (d->x[2] >> 31);
        d->x[2] = (b2 << 31) | (d->x[2] & 0x7fffffff);
        b2 = (b2 >> 1) & 1;
        dst->x[2] = d->x[2];
        
        d->x[3] = (src1->x[3] & 0x7fffffff) - (src2->x[3] & 0x7fffffff) - b2;
        uint32_t b3 = (src1->x[3] >> 31) - (src2->x[3] >> 31) - (d->x[3] >> 31);
        d->x[3] = (b3 << 31) | (d->x[3] & 0x7fffffff);
        b3 = (b3 >> 1) & 1;
        dst->x[3] = d->x[3];
        
        d->x[4] = (src1->x[4] & 0x7fffffff) - (src2->x[4] & 0x7fffffff) - b3;
        uint32_t b4 = (src1->x[4] >> 31) - (src2->x[4] >> 31) - (d->x[4] >> 31);
        d->x[4] = (b4 << 31) | (d->x[4] & 0x7fffffff);
        b4 = (b4 >> 1) & 1;
        dst->x[4] = d->x[4];
        
    #endif
    return;
}
/*
 * Mul two ul160's
 */
inline void ul160_mul(ul160 dst, ul160 src1, ul160 src2) {
    #if defined(UL_NVIDIA)
        asm(
          "mul.lo.u32 %0, %5, %10;\n\t"
          "mul.hi.u32 %1, %5, %10;\n\t"
          "mad.lo.cc.u32 %1, %5, %11, %1;\n\t"
          "madc.hi.u32 %2, %5, %11, 0;\n\t"
          "mad.lo.cc.u32 %1, %6, %10, %1;\n\t"
          "madc.hi.cc.u32 %2, %6, %10, %2;\n\t"
          "madc.lo.u32 %3, %5, %13, 0;\n\t"
          "mad.lo.cc.u32 %2, %5, %12, %2;\n\t"
          "madc.hi.cc.u32 %3, %5, %12, %3;\n\t"
          "madc.lo.u32 %4, %5, %14, 0;\n\t"
          "mad.lo.cc.u32 %2, %6, %11, %2;\n\t"
          "madc.hi.cc.u32 %3, %6, %11, %3;\n\t"
          "madc.lo.u32 %4, %6, %13, %4;\n\t"
          "mad.lo.cc.u32 %2, %7, %10, %2;\n\t"
          "madc.hi.cc.u32 %3, %7, %10, %3;\n\t"
          "madc.lo.u32 %4, %7, %12, %4;\n\t"
          "mad.lo.cc.u32 %3, %6, %12, %3;\n\t"
          "madc.hi.u32 %4, %6, %12, %4;\n\t"
          "mad.lo.cc.u32 %3, %7, %11, %3;\n\t"
          "madc.hi.u32 %4, %7, %11, %4;\n\t"
          "mad.lo.cc.u32 %3, %8, %10, %3;\n\t"
          "madc.hi.u32 %4, %8, %10, %4;\n\t"
          "mad.lo.u32 %4, %8, %11, %4;\n\t"
          "mad.lo.u32 %4, %9, %10, %4;\n\t"
          "mad.hi.u32 %4, %5, %13, %4;\n\t"
          : "=r" (dst->x[0]), "=r" (dst->x[1]), "=r" (dst->x[2]), "=r" (dst->x[3]), "=r" (dst->x[4])
          : "r" (src1->x[0]), "r" (src1->x[1]), "r" (src1->x[2]), "r" (src1->x[3]), "r" (src1->x[4]), 
            "r" (src2->x[0]), "r" (src2->x[1]), "r" (src2->x[2]), "r" (src2->x[3]), "r" (src2->x[4])
          : "cc"
        );
    #else
        ul160 d = {0};
        
        uint64_t tmp0 = ((uint64_t)src1->x[0]) * ((uint64_t)src2->x[0]);
        ul160 tmp0_;
        tmp0_->x[0] = tmp0;
        tmp0_->x[1] = tmp0 >> 32;
        tmp0_->x[2] = 0;
        tmp0_->x[3] = 0;
        tmp0_->x[4] = 0;
        ul160_add(d, tmp0_, d);
        
        uint64_t tmp1 = ((uint64_t)src1->x[0]) * ((uint64_t)src2->x[1]);
        ul160 tmp1_;
        tmp1_->x[0] = 0;
        tmp1_->x[1] = tmp1;
        tmp1_->x[2] = tmp1 >> 32;
        tmp1_->x[3] = 0;
        tmp1_->x[4] = 0;
        ul160_add(d, tmp1_, d);
        
        uint64_t tmp2 = ((uint64_t)src1->x[1]) * ((uint64_t)src2->x[0]);
        ul160 tmp2_;
        tmp2_->x[0] = 0;
        tmp2_->x[1] = tmp2;
        tmp2_->x[2] = tmp2 >> 32;
        tmp2_->x[3] = 0;
        tmp2_->x[4] = 0;
        ul160_add(d, tmp2_, d);
        
        uint64_t tmp3 = ((uint64_t)src1->x[0]) * ((uint64_t)src2->x[2]);
        ul160 tmp3_;
        tmp3_->x[0] = 0;
        tmp3_->x[1] = 0;
        tmp3_->x[2] = tmp3;
        tmp3_->x[3] = tmp3 >> 32;
        tmp3_->x[4] = 0;
        ul160_add(d, tmp3_, d);
        
        uint64_t tmp4 = ((uint64_t)src1->x[1]) * ((uint64_t)src2->x[1]);
        ul160 tmp4_;
        tmp4_->x[0] = 0;
        tmp4_->x[1] = 0;
        tmp4_->x[2] = tmp4;
        tmp4_->x[3] = tmp4 >> 32;
        tmp4_->x[4] = 0;
        ul160_add(d, tmp4_, d);
        
        uint64_t tmp5 = ((uint64_t)src1->x[2]) * ((uint64_t)src2->x[0]);
        ul160 tmp5_;
        tmp5_->x[0] = 0;
        tmp5_->x[1] = 0;
        tmp5_->x[2] = tmp5;
        tmp5_->x[3] = tmp5 >> 32;
        tmp5_->x[4] = 0;
        ul160_add(d, tmp5_, d);
        
        uint64_t tmp6 = ((uint64_t)src1->x[0]) * ((uint64_t)src2->x[3]);
        ul160 tmp6_;
        tmp6_->x[0] = 0;
        tmp6_->x[1] = 0;
        tmp6_->x[2] = 0;
        tmp6_->x[3] = tmp6;
        tmp6_->x[4] = tmp6 >> 32;
        ul160_add(d, tmp6_, d);
        
        uint64_t tmp7 = ((uint64_t)src1->x[1]) * ((uint64_t)src2->x[2]);
        ul160 tmp7_;
        tmp7_->x[0] = 0;
        tmp7_->x[1] = 0;
        tmp7_->x[2] = 0;
        tmp7_->x[3] = tmp7;
        tmp7_->x[4] = tmp7 >> 32;
        ul160_add(d, tmp7_, d);
        
        uint64_t tmp8 = ((uint64_t)src1->x[2]) * ((uint64_t)src2->x[1]);
        ul160 tmp8_;
        tmp8_->x[0] = 0;
        tmp8_->x[1] = 0;
        tmp8_->x[2] = 0;
        tmp8_->x[3] = tmp8;
        tmp8_->x[4] = tmp8 >> 32;
        ul160_add(d, tmp8_, d);
        
        uint64_t tmp9 = ((uint64_t)src1->x[3]) * ((uint64_t)src2->x[0]);
        ul160 tmp9_;
        tmp9_->x[0] = 0;
        tmp9_->x[1] = 0;
        tmp9_->x[2] = 0;
        tmp9_->x[3] = tmp9;
        tmp9_->x[4] = tmp9 >> 32;
        ul160_add(d, tmp9_, d);
        
        uint64_t tmp10 = ((uint64_t)src1->x[0]) * ((uint64_t)src2->x[4]);
        ul160 tmp10_;
        tmp10_->x[0] = 0;
        tmp10_->x[1] = 0;
        tmp10_->x[2] = 0;
        tmp10_->x[3] = 0;
        tmp10_->x[4] = tmp10;
        ul160_add(d, tmp10_, d);
        
        uint64_t tmp11 = ((uint64_t)src1->x[1]) * ((uint64_t)src2->x[3]);
        ul160 tmp11_;
        tmp11_->x[0] = 0;
        tmp11_->x[1] = 0;
        tmp11_->x[2] = 0;
        tmp11_->x[3] = 0;
        tmp11_->x[4] = tmp11;
        ul160_add(d, tmp11_, d);
        
        uint64_t tmp12 = ((uint64_t)src1->x[2]) * ((uint64_t)src2->x[2]);
        ul160 tmp12_;
        tmp12_->x[0] = 0;
        tmp12_->x[1] = 0;
        tmp12_->x[2] = 0;
        tmp12_->x[3] = 0;
        tmp12_->x[4] = tmp12;
        ul160_add(d, tmp12_, d);
        
        uint64_t tmp13 = ((uint64_t)src1->x[3]) * ((uint64_t)src2->x[1]);
        ul160 tmp13_;
        tmp13_->x[0] = 0;
        tmp13_->x[1] = 0;
        tmp13_->x[2] = 0;
        tmp13_->x[3] = 0;
        tmp13_->x[4] = tmp13;
        ul160_add(d, tmp13_, d);
        
        uint64_t tmp14 = ((uint64_t)src1->x[4]) * ((uint64_t)src2->x[0]);
        ul160 tmp14_;
        tmp14_->x[0] = 0;
        tmp14_->x[1] = 0;
        tmp14_->x[2] = 0;
        tmp14_->x[3] = 0;
        tmp14_->x[4] = tmp14;
        ul160_add(d, tmp14_, d);
        
        dst->x[0] = d->x[0];
        dst->x[1] = d->x[1];
        dst->x[2] = d->x[2];
        dst->x[3] = d->x[3];
        dst->x[4] = d->x[4];
    #endif
    return;
}




/*
 * Initialize mod160
 */
inline void mod160_init(mod160 n) {
}

/*
 * Add two ul160's modulo another
 */
inline void ul160_modadd(ul160 dst, ul160 src1, ul160 src2, mod160 n) {
    ul160_add(dst, src1, src2);
    if (ul160_cmp(dst, n->n) >= 0)
        ul160_sub(dst, dst, n->n);
}

/*
 * Subtract one ul160 from another modulo a third
 */
inline void ul160_modsub(ul160 dst, ul160 src1, ul160 src2, mod160 n) {
    ul160 tr1, tr2;
    ul160_sub(tr1, src1, src2);
    ul160_add(tr2, tr1, n->n);
    if (ul160_cmp(src1, src2) >= 0)
        ul160_set(dst, tr1);
    else
        ul160_set(dst, tr2);
}

/*
 * Mul two ul160's modulo a third, followed by Montgomery reduction
 */
void ul160_modmul(ul160 _dst, ul160 _src1, ul160 _src2, mod160 n) {
    #if defined(UL_NVIDIA)
        volatile ul160 src1;
        volatile ul160 src2;
        /* ul160_set(src1, _src1); */
        src1->x[0] = _src1->x[0];
        src1->x[1] = _src1->x[1];
        src1->x[2] = _src1->x[2];
        src1->x[3] = _src1->x[3];
        src1->x[4] = _src1->x[4];
        /* ul160_set(src2, _src2); */
        src2->x[0] = _src2->x[0];
        src2->x[1] = _src2->x[1];
        src2->x[2] = _src2->x[2];
        src2->x[3] = _src2->x[3];
        src2->x[4] = _src2->x[4];
        
        uint32_t q = 0;
        ul160 dst = { 0 };
        uint32_t dst_5 = 0;
        
        asm(
          /* Compute c_0..4 for the product a*b, with carry-out to dst_5 */
          "mad.lo.cc.u32  %0, %7, %12, %0;\n\t"  /* c_0 += lo(a_0, b_0) */
          "madc.hi.cc.u32 %1, %7, %12, %1;\n\t"  /* c_1 += hi(a_0, b_0) */
          "madc.lo.cc.u32 %2, %7, %14, %2;\n\t"  /* c_2 += lo(a_0, b_2) */
          "madc.hi.cc.u32 %3, %7, %14, %3;\n\t"  /* c_3 += hi(a_0, b_2) */
          "madc.lo.cc.u32 %4, %7, %16, %4;\n\t"  /* c_4 += lo(a_0, b_4) */
          "addc.u32 %5, %5, 0;\n\t"  /* accum carry in c_5 */
          "mad.lo.cc.u32  %1, %7, %13, %1;\n\t"  /* c_1 += lo(a_0, b_1) */
          "madc.hi.cc.u32 %2, %7, %13, %2;\n\t"  /* c_2 += hi(a_0, b_1) */
          "madc.lo.cc.u32 %3, %7, %15, %3;\n\t"  /* c_3 += lo(a_0, b_3) */
          "madc.hi.cc.u32 %4, %7, %15, %4;\n\t"  /* c_4 += hi(a_0, b_3) */
          "addc.u32 %5, %5, 0;\n\t"  /* accum carry in c_5 */
          "mad.lo.cc.u32  %1, %8, %12, %1;\n\t"  /* c_1 += lo(a_1, b_0) */
          "madc.hi.cc.u32 %2, %8, %12, %2;\n\t"  /* c_2 += hi(a_1, b_0) */
          "madc.lo.cc.u32 %3, %8, %14, %3;\n\t"  /* c_3 += lo(a_1, b_2) */
          "madc.hi.cc.u32 %4, %8, %14, %4;\n\t"  /* c_4 += hi(a_1, b_2) */
          "addc.u32 %5, %5, 0;\n\t"  /* accum carry in c_5 */
          "mad.lo.cc.u32  %2, %8, %13, %2;\n\t"  /* c_2 += lo(a_1, b_1) */
          "madc.hi.cc.u32 %3, %8, %13, %3;\n\t"  /* c_3 += hi(a_1, b_1) */
          "madc.lo.cc.u32 %4, %8, %15, %4;\n\t"  /* c_4 += lo(a_1, b_3) */
          "addc.u32 %5, %5, 0;\n\t"  /* accum carry in c_5 */
          "mad.lo.cc.u32  %2, %9, %12, %2;\n\t"  /* c_2 += lo(a_2, b_0) */
          "madc.hi.cc.u32 %3, %9, %12, %3;\n\t"  /* c_3 += hi(a_2, b_0) */
          "madc.lo.cc.u32 %4, %9, %14, %4;\n\t"  /* c_4 += lo(a_2, b_2) */
          "addc.u32 %5, %5, 0;\n\t"  /* accum carry in c_5 */
          "mad.lo.cc.u32  %3, %9, %13, %3;\n\t"  /* c_3 += lo(a_2, b_1) */
          "madc.hi.cc.u32 %4, %9, %13, %4;\n\t"  /* c_4 += hi(a_2, b_1) */
          "addc.u32 %5, %5, 0;\n\t"  /* accum carry in c_5 */
          "mad.lo.cc.u32  %3, %10, %12, %3;\n\t"  /* c_3 += lo(a_3, b_0) */
          "madc.hi.cc.u32 %4, %10, %12, %4;\n\t"  /* c_4 += hi(a_3, b_0) */
          "addc.u32 %5, %5, 0;\n\t"  /* accum carry in c_5 */
          "mad.lo.cc.u32  %4, %10, %13, %4;\n\t"  /* c_4 += lo(a_3, b_1) */
          "addc.u32 %5, %5, 0;\n\t"  /* accum carry in c_5 */
          "mad.lo.cc.u32  %4, %11, %12, %4;\n\t"  /* c_4 += lo(a_4, b_0) */
          "addc.u32 %5, %5, 0;\n\t"  /* accum carry in c_5 */
          
          /* Add in the qN's */
          /* n = 0... */
            /* Compute q = mu * c_0 */
            "mov.u32 %6, %0;\n\t"
            "mul.lo.u32 %6, %6, %22;\n\t"
            /* Update c_0 with qN_0 */
            "mad.lo.cc.u32 %0, %6, %17, %0;\n\t"  /* c_0 += lo(q, n_0) */
            /* Shift */
            "mov.u32 %0, %1;\n\t"  /* dst_0 <- c_1 */
            "mov.u32 %1, %2;\n\t"  /* dst_1 <- c_2 */
            "mov.u32 %2, %3;\n\t"  /* dst_2 <- c_3 */
            "mov.u32 %3, %4;\n\t"  /* dst_3 <- c_4 */
            "mov.u32 %4, %5;\n\t"  /* dst_4 <- c_5 */
            "xor.b32 %5, %5, %5;\n\t"  /* dst_5 <- c_6 */
            /* Compute and add-in qN, with carry-out to dst_5 */
            "madc.hi.cc.u32 %0, %6, %17, %0;\n\t"  /* c_1 += hi(q, n_0) */
            "madc.lo.cc.u32 %1, %6, %19, %1;\n\t"  /* c_2 += lo(q, n_2) */
            "madc.hi.cc.u32 %2, %6, %19, %2;\n\t"  /* c_3 += hi(q, n_2) */
            "madc.lo.cc.u32 %3, %6, %21, %3;\n\t"  /* c_4 += lo(q, n_4) */
            "madc.hi.cc.u32 %4, %6, %21, %4;\n\t"  /* c_5 += hi(q, n_4) */
            "addc.u32    %5, %5, 0;\n\t"  /* accum carry in dst_5 */
            "mad.lo.cc.u32  %0, %6, %18, %0;\n\t"  /* c_1 += lo(q, n_1) */
            "madc.hi.cc.u32 %1, %6, %18, %1;\n\t"  /* c_2 += hi(q, n_1) */
            "madc.lo.cc.u32 %2, %6, %20, %2;\n\t"  /* c_3 += lo(q, n_3) */
            "madc.hi.cc.u32 %3, %6, %20, %3;\n\t"  /* c_4 += hi(q, n_3) */
            "addc.cc.u32 %4, %4, 0;\n\t"  /* accum carry in dst_4 */
            "addc.u32    %5, %5, 0;\n\t"  /* accum carry in dst_5 */
          /* n = 1... */
            /* Compute q = mu * c_1 */
            "mov.u32 %6, %0;\n\t"
            "mul.lo.u32 %6, %6, %22;\n\t"
            /* Update c_1 with qN_0 */
            "mad.lo.cc.u32 %0, %6, %17, %0;\n\t"  /* c_1 += lo(q, n_0) */
            /* Shift */
            "mov.u32 %0, %1;\n\t"  /* dst_0 <- c_2 */
            "mov.u32 %1, %2;\n\t"  /* dst_1 <- c_3 */
            "mov.u32 %2, %3;\n\t"  /* dst_2 <- c_4 */
            "mov.u32 %3, %4;\n\t"  /* dst_3 <- c_5 */
            "mov.u32 %4, %5;\n\t"  /* dst_4 <- c_6 */
            "xor.b32 %5, %5, %5;\n\t"  /* dst_5 <- c_7 */
            /* Compute and add-in qN, with carry-out to dst_5 */
            "madc.hi.cc.u32 %0, %6, %17, %0;\n\t"  /* c_2 += hi(q, n_0) */
            "madc.lo.cc.u32 %1, %6, %19, %1;\n\t"  /* c_3 += lo(q, n_2) */
            "madc.hi.cc.u32 %2, %6, %19, %2;\n\t"  /* c_4 += hi(q, n_2) */
            "madc.lo.cc.u32 %3, %6, %21, %3;\n\t"  /* c_5 += lo(q, n_4) */
            "madc.hi.cc.u32 %4, %6, %21, %4;\n\t"  /* c_6 += hi(q, n_4) */
            "addc.u32    %5, %5, 0;\n\t"  /* accum carry in dst_5 */
            "mad.lo.cc.u32  %0, %6, %18, %0;\n\t"  /* c_1 += lo(q, n_1) */
            "madc.hi.cc.u32 %1, %6, %18, %1;\n\t"  /* c_2 += hi(q, n_1) */
            "madc.lo.cc.u32 %2, %6, %20, %2;\n\t"  /* c_3 += lo(q, n_3) */
            "madc.hi.cc.u32 %3, %6, %20, %3;\n\t"  /* c_4 += hi(q, n_3) */
            "addc.cc.u32 %4, %4, 0;\n\t"  /* accum carry in dst_4 */
            "addc.u32    %5, %5, 0;\n\t"  /* accum carry in dst_5 */
          /* n = 2... */
            /* Compute q = mu * c_2 */
            "mov.u32 %6, %0;\n\t"
            "mul.lo.u32 %6, %6, %22;\n\t"
            /* Update c_2 with qN_0 */
            "mad.lo.cc.u32 %0, %6, %17, %0;\n\t"  /* c_2 += lo(q, n_0) */
            /* Shift */
            "mov.u32 %0, %1;\n\t"  /* dst_0 <- c_3 */
            "mov.u32 %1, %2;\n\t"  /* dst_1 <- c_4 */
            "mov.u32 %2, %3;\n\t"  /* dst_2 <- c_5 */
            "mov.u32 %3, %4;\n\t"  /* dst_3 <- c_6 */
            "mov.u32 %4, %5;\n\t"  /* dst_4 <- c_7 */
            "xor.b32 %5, %5, %5;\n\t"  /* dst_5 <- c_8 */
            /* Compute and add-in qN, with carry-out to dst_5 */
            "madc.hi.cc.u32 %0, %6, %17, %0;\n\t"  /* c_3 += hi(q, n_0) */
            "madc.lo.cc.u32 %1, %6, %19, %1;\n\t"  /* c_4 += lo(q, n_2) */
            "madc.hi.cc.u32 %2, %6, %19, %2;\n\t"  /* c_5 += hi(q, n_2) */
            "madc.lo.cc.u32 %3, %6, %21, %3;\n\t"  /* c_6 += lo(q, n_4) */
            "madc.hi.cc.u32 %4, %6, %21, %4;\n\t"  /* c_7 += hi(q, n_4) */
            "addc.u32    %5, %5, 0;\n\t"  /* accum carry in dst_5 */
            "mad.lo.cc.u32  %0, %6, %18, %0;\n\t"  /* c_1 += lo(q, n_1) */
            "madc.hi.cc.u32 %1, %6, %18, %1;\n\t"  /* c_2 += hi(q, n_1) */
            "madc.lo.cc.u32 %2, %6, %20, %2;\n\t"  /* c_3 += lo(q, n_3) */
            "madc.hi.cc.u32 %3, %6, %20, %3;\n\t"  /* c_4 += hi(q, n_3) */
            "addc.cc.u32 %4, %4, 0;\n\t"  /* accum carry in dst_4 */
            "addc.u32    %5, %5, 0;\n\t"  /* accum carry in dst_5 */
          /* n = 3... */
            /* Compute q = mu * c_3 */
            "mov.u32 %6, %0;\n\t"
            "mul.lo.u32 %6, %6, %22;\n\t"
            /* Update c_3 with qN_0 */
            "mad.lo.cc.u32 %0, %6, %17, %0;\n\t"  /* c_3 += lo(q, n_0) */
            /* Shift */
            "mov.u32 %0, %1;\n\t"  /* dst_0 <- c_4 */
            "mov.u32 %1, %2;\n\t"  /* dst_1 <- c_5 */
            "mov.u32 %2, %3;\n\t"  /* dst_2 <- c_6 */
            "mov.u32 %3, %4;\n\t"  /* dst_3 <- c_7 */
            "mov.u32 %4, %5;\n\t"  /* dst_4 <- c_8 */
            "xor.b32 %5, %5, %5;\n\t"  /* dst_5 <- c_9 */
            /* Compute and add-in qN, with carry-out to dst_5 */
            "madc.hi.cc.u32 %0, %6, %17, %0;\n\t"  /* c_4 += hi(q, n_0) */
            "madc.lo.cc.u32 %1, %6, %19, %1;\n\t"  /* c_5 += lo(q, n_2) */
            "madc.hi.cc.u32 %2, %6, %19, %2;\n\t"  /* c_6 += hi(q, n_2) */
            "madc.lo.cc.u32 %3, %6, %21, %3;\n\t"  /* c_7 += lo(q, n_4) */
            "madc.hi.cc.u32 %4, %6, %21, %4;\n\t"  /* c_8 += hi(q, n_4) */
            "addc.u32    %5, %5, 0;\n\t"  /* accum carry in dst_5 */
            "mad.lo.cc.u32  %0, %6, %18, %0;\n\t"  /* c_1 += lo(q, n_1) */
            "madc.hi.cc.u32 %1, %6, %18, %1;\n\t"  /* c_2 += hi(q, n_1) */
            "madc.lo.cc.u32 %2, %6, %20, %2;\n\t"  /* c_3 += lo(q, n_3) */
            "madc.hi.cc.u32 %3, %6, %20, %3;\n\t"  /* c_4 += hi(q, n_3) */
            "addc.cc.u32 %4, %4, 0;\n\t"  /* accum carry in dst_4 */
            "addc.u32    %5, %5, 0;\n\t"  /* accum carry in dst_5 */
          /* n = 4... */
            /* Compute q = mu * c_4 */
            "mov.u32 %6, %0;\n\t"
            "mul.lo.u32 %6, %6, %22;\n\t"
            /* Update c_4 with qN_0 */
            "mad.lo.cc.u32 %0, %6, %17, %0;\n\t"  /* c_4 += lo(q, n_0) */
            /* Shift */
            "mov.u32 %0, %1;\n\t"  /* dst_0 <- c_5 */
            "mov.u32 %1, %2;\n\t"  /* dst_1 <- c_6 */
            "mov.u32 %2, %3;\n\t"  /* dst_2 <- c_7 */
            "mov.u32 %3, %4;\n\t"  /* dst_3 <- c_8 */
            "mov.u32 %4, %5;\n\t"  /* dst_4 <- c_9 */
            "xor.b32 %5, %5, %5;\n\t"  /* dst_5 <- c_10 */
            /* Compute and add-in qN, with carry-out to dst_5 */
            "madc.hi.cc.u32 %0, %6, %17, %0;\n\t"  /* c_5 += hi(q, n_0) */
            "madc.lo.cc.u32 %1, %6, %19, %1;\n\t"  /* c_6 += lo(q, n_2) */
            "madc.hi.cc.u32 %2, %6, %19, %2;\n\t"  /* c_7 += hi(q, n_2) */
            "madc.lo.cc.u32 %3, %6, %21, %3;\n\t"  /* c_8 += lo(q, n_4) */
            "madc.hi.cc.u32 %4, %6, %21, %4;\n\t"  /* c_9 += hi(q, n_4) */
            "addc.u32    %5, %5, 0;\n\t"  /* accum carry in dst_5 */
            "mad.lo.cc.u32  %0, %6, %18, %0;\n\t"  /* c_1 += lo(q, n_1) */
            "madc.hi.cc.u32 %1, %6, %18, %1;\n\t"  /* c_2 += hi(q, n_1) */
            "madc.lo.cc.u32 %2, %6, %20, %2;\n\t"  /* c_3 += lo(q, n_3) */
            "madc.hi.cc.u32 %3, %6, %20, %3;\n\t"  /* c_4 += hi(q, n_3) */
            "addc.cc.u32 %4, %4, 0;\n\t"  /* accum carry in dst_4 */
            "addc.u32    %5, %5, 0;\n\t"  /* accum carry in dst_5 */
          
          /* Compute c_5..9 in the product a*b, storing the result in dst_0..4, with carry-out to dst_5 */
          "mad.hi.cc.u32  %0, %7, %16, %0;\n\t"  /* c_5 += hi(a_0, b_4) */
          "addc.cc.u32 %1, %1, 0;\n\t"  /* accum carry in c_6 */
          "addc.cc.u32 %2, %2, 0;\n\t"  /* accum carry in c_7 */
          "addc.cc.u32 %3, %3, 0;\n\t"  /* accum carry in c_8 */
          "addc.cc.u32 %4, %4, 0;\n\t"  /* accum carry in c_9 */
          "addc.u32    %5, %5, 0;\n\t"  /* accum carry in c_10 */
          "mad.lo.cc.u32  %0, %8, %16, %0;\n\t"  /* c_5 += lo(a_1, b_4) */
          "madc.hi.cc.u32 %1, %8, %16, %1;\n\t"  /* c_6 += hi(a_1, b_4) */
          "addc.cc.u32 %2, %2, 0;\n\t"  /* accum carry in c_7 */
          "addc.cc.u32 %3, %3, 0;\n\t"  /* accum carry in c_8 */
          "addc.cc.u32 %4, %4, 0;\n\t"  /* accum carry in c_9 */
          "addc.u32    %5, %5, 0;\n\t"  /* accum carry in c_10 */
          "mad.hi.cc.u32  %0, %8, %15, %0;\n\t"  /* c_5 += hi(a_1, b_3) */
          "addc.cc.u32 %1, %1, 0;\n\t"  /* accum carry in c_6 */
          "addc.cc.u32 %2, %2, 0;\n\t"  /* accum carry in c_7 */
          "addc.cc.u32 %3, %3, 0;\n\t"  /* accum carry in c_8 */
          "addc.cc.u32 %4, %4, 0;\n\t"  /* accum carry in c_9 */
          "addc.u32    %5, %5, 0;\n\t"  /* accum carry in c_10 */
          "mad.hi.cc.u32  %0, %9, %14, %0;\n\t"  /* c_5 += hi(a_2, b_2) */
          "madc.lo.cc.u32 %1, %9, %16, %1;\n\t"  /* c_6 += lo(a_2, b_4) */
          "madc.hi.cc.u32 %2, %9, %16, %2;\n\t"  /* c_7 += hi(a_2, b_4) */
          "addc.cc.u32 %3, %3, 0;\n\t"  /* accum carry in c_8 */
          "addc.cc.u32 %4, %4, 0;\n\t"  /* accum carry in c_9 */
          "addc.u32    %5, %5, 0;\n\t"  /* accum carry in c_10 */
          "mad.lo.cc.u32  %0, %9, %15, %0;\n\t"  /* c_5 += lo(a_2, b_3) */
          "madc.hi.cc.u32 %1, %9, %15, %1;\n\t"  /* c_6 += hi(a_2, b_3) */
          "addc.cc.u32 %2, %2, 0;\n\t"  /* accum carry in c_7 */
          "addc.cc.u32 %3, %3, 0;\n\t"  /* accum carry in c_8 */
          "addc.cc.u32 %4, %4, 0;\n\t"  /* accum carry in c_9 */
          "addc.u32    %5, %5, 0;\n\t"  /* accum carry in c_10 */
          "mad.lo.cc.u32  %0, %10, %14, %0;\n\t"  /* c_5 += lo(a_3, b_2) */
          "madc.hi.cc.u32 %1, %10, %14, %1;\n\t"  /* c_6 += hi(a_3, b_2) */
          "madc.lo.cc.u32 %2, %10, %16, %2;\n\t"  /* c_7 += lo(a_3, b_4) */
          "madc.hi.cc.u32 %3, %10, %16, %3;\n\t"  /* c_8 += hi(a_3, b_4) */
          "addc.cc.u32 %4, %4, 0;\n\t"  /* accum carry in c_9 */
          "addc.u32    %5, %5, 0;\n\t"  /* accum carry in c_10 */
          "mad.hi.cc.u32  %0, %10, %13, %0;\n\t"  /* c_5 += hi(a_3, b_1) */
          "madc.lo.cc.u32 %1, %10, %15, %1;\n\t"  /* c_6 += lo(a_3, b_3) */
          "madc.hi.cc.u32 %2, %10, %15, %2;\n\t"  /* c_7 += hi(a_3, b_3) */
          "addc.cc.u32 %3, %3, 0;\n\t"  /* accum carry in c_8 */
          "addc.cc.u32 %4, %4, 0;\n\t"  /* accum carry in c_9 */
          "addc.u32    %5, %5, 0;\n\t"  /* accum carry in c_10 */
          "mad.hi.cc.u32  %0, %11, %12, %0;\n\t"  /* c_5 += hi(a_4, b_0) */
          "madc.lo.cc.u32 %1, %11, %14, %1;\n\t"  /* c_6 += lo(a_4, b_2) */
          "madc.hi.cc.u32 %2, %11, %14, %2;\n\t"  /* c_7 += hi(a_4, b_2) */
          "madc.lo.cc.u32 %3, %11, %16, %3;\n\t"  /* c_8 += lo(a_4, b_4) */
          "madc.hi.cc.u32 %4, %11, %16, %4;\n\t"  /* c_9 += hi(a_4, b_4) */
          "addc.u32    %5, %5, 0;\n\t"  /* accum carry in c_10 */
          "mad.lo.cc.u32  %0, %11, %13, %0;\n\t"  /* c_5 += lo(a_4, b_1) */
          "madc.hi.cc.u32 %1, %11, %13, %1;\n\t"  /* c_6 += hi(a_4, b_1) */
          "madc.lo.cc.u32 %2, %11, %15, %2;\n\t"  /* c_7 += lo(a_4, b_3) */
          "madc.hi.cc.u32 %3, %11, %15, %3;\n\t"  /* c_8 += hi(a_4, b_3) */
          "addc.cc.u32 %4, %4, 0;\n\t"  /* accum carry in c_9 */
          "addc.u32    %5, %5, 0;\n\t"  /* accum carry in c_10 */
          
          : "+r" (dst->x[0]), "+r" (dst->x[1]), "+r" (dst->x[2]), "+r" (dst->x[3]), "+r" (dst->x[4]), "+r" (dst_5), "+r" (q)
          : "r" (src1->x[0]), "r" (src1->x[1]), "r" (src1->x[2]), "r" (src1->x[3]), "r" (src1->x[4]), 
            "r" (src2->x[0]), "r" (src2->x[1]), "r" (src2->x[2]), "r" (src2->x[3]), "r" (src2->x[4]), 
            "r" (n->n->x[0]), "r" (n->n->x[1]), "r" (n->n->x[2]), "r" (n->n->x[3]), "r" (n->n->x[4]), 
            "r" (n->np)
        );
        
        /* Reduce as needed */
        if (dst_5 || (ul160_cmp(dst, n->n) >= 0))
            ul160_sub(dst, dst, n->n);
        ul160_set(_dst, dst);
    #else
        /* The pairwise products of the a_i and b_j */
        const uint64_t a_0__b_0 = ((uint64_t)_src1->x[0]) * ((uint64_t)_src2->x[0]);
        const uint32_t a_0__b_0_lo = a_0__b_0;
        const uint32_t a_0__b_0_hi = a_0__b_0 >> 32;
        const uint64_t a_0__b_1 = ((uint64_t)_src1->x[0]) * ((uint64_t)_src2->x[1]);
        const uint32_t a_0__b_1_lo = a_0__b_1;
        const uint32_t a_0__b_1_hi = a_0__b_1 >> 32;
        const uint64_t a_0__b_2 = ((uint64_t)_src1->x[0]) * ((uint64_t)_src2->x[2]);
        const uint32_t a_0__b_2_lo = a_0__b_2;
        const uint32_t a_0__b_2_hi = a_0__b_2 >> 32;
        const uint64_t a_0__b_3 = ((uint64_t)_src1->x[0]) * ((uint64_t)_src2->x[3]);
        const uint32_t a_0__b_3_lo = a_0__b_3;
        const uint32_t a_0__b_3_hi = a_0__b_3 >> 32;
        const uint64_t a_0__b_4 = ((uint64_t)_src1->x[0]) * ((uint64_t)_src2->x[4]);
        const uint32_t a_0__b_4_lo = a_0__b_4;
        const uint32_t a_0__b_4_hi = a_0__b_4 >> 32;
        const uint64_t a_1__b_0 = ((uint64_t)_src1->x[1]) * ((uint64_t)_src2->x[0]);
        const uint32_t a_1__b_0_lo = a_1__b_0;
        const uint32_t a_1__b_0_hi = a_1__b_0 >> 32;
        const uint64_t a_1__b_1 = ((uint64_t)_src1->x[1]) * ((uint64_t)_src2->x[1]);
        const uint32_t a_1__b_1_lo = a_1__b_1;
        const uint32_t a_1__b_1_hi = a_1__b_1 >> 32;
        const uint64_t a_1__b_2 = ((uint64_t)_src1->x[1]) * ((uint64_t)_src2->x[2]);
        const uint32_t a_1__b_2_lo = a_1__b_2;
        const uint32_t a_1__b_2_hi = a_1__b_2 >> 32;
        const uint64_t a_1__b_3 = ((uint64_t)_src1->x[1]) * ((uint64_t)_src2->x[3]);
        const uint32_t a_1__b_3_lo = a_1__b_3;
        const uint32_t a_1__b_3_hi = a_1__b_3 >> 32;
        const uint64_t a_1__b_4 = ((uint64_t)_src1->x[1]) * ((uint64_t)_src2->x[4]);
        const uint32_t a_1__b_4_lo = a_1__b_4;
        const uint32_t a_1__b_4_hi = a_1__b_4 >> 32;
        const uint64_t a_2__b_0 = ((uint64_t)_src1->x[2]) * ((uint64_t)_src2->x[0]);
        const uint32_t a_2__b_0_lo = a_2__b_0;
        const uint32_t a_2__b_0_hi = a_2__b_0 >> 32;
        const uint64_t a_2__b_1 = ((uint64_t)_src1->x[2]) * ((uint64_t)_src2->x[1]);
        const uint32_t a_2__b_1_lo = a_2__b_1;
        const uint32_t a_2__b_1_hi = a_2__b_1 >> 32;
        const uint64_t a_2__b_2 = ((uint64_t)_src1->x[2]) * ((uint64_t)_src2->x[2]);
        const uint32_t a_2__b_2_lo = a_2__b_2;
        const uint32_t a_2__b_2_hi = a_2__b_2 >> 32;
        const uint64_t a_2__b_3 = ((uint64_t)_src1->x[2]) * ((uint64_t)_src2->x[3]);
        const uint32_t a_2__b_3_lo = a_2__b_3;
        const uint32_t a_2__b_3_hi = a_2__b_3 >> 32;
        const uint64_t a_2__b_4 = ((uint64_t)_src1->x[2]) * ((uint64_t)_src2->x[4]);
        const uint32_t a_2__b_4_lo = a_2__b_4;
        const uint32_t a_2__b_4_hi = a_2__b_4 >> 32;
        const uint64_t a_3__b_0 = ((uint64_t)_src1->x[3]) * ((uint64_t)_src2->x[0]);
        const uint32_t a_3__b_0_lo = a_3__b_0;
        const uint32_t a_3__b_0_hi = a_3__b_0 >> 32;
        const uint64_t a_3__b_1 = ((uint64_t)_src1->x[3]) * ((uint64_t)_src2->x[1]);
        const uint32_t a_3__b_1_lo = a_3__b_1;
        const uint32_t a_3__b_1_hi = a_3__b_1 >> 32;
        const uint64_t a_3__b_2 = ((uint64_t)_src1->x[3]) * ((uint64_t)_src2->x[2]);
        const uint32_t a_3__b_2_lo = a_3__b_2;
        const uint32_t a_3__b_2_hi = a_3__b_2 >> 32;
        const uint64_t a_3__b_3 = ((uint64_t)_src1->x[3]) * ((uint64_t)_src2->x[3]);
        const uint32_t a_3__b_3_lo = a_3__b_3;
        const uint32_t a_3__b_3_hi = a_3__b_3 >> 32;
        const uint64_t a_3__b_4 = ((uint64_t)_src1->x[3]) * ((uint64_t)_src2->x[4]);
        const uint32_t a_3__b_4_lo = a_3__b_4;
        const uint32_t a_3__b_4_hi = a_3__b_4 >> 32;
        const uint64_t a_4__b_0 = ((uint64_t)_src1->x[4]) * ((uint64_t)_src2->x[0]);
        const uint32_t a_4__b_0_lo = a_4__b_0;
        const uint32_t a_4__b_0_hi = a_4__b_0 >> 32;
        const uint64_t a_4__b_1 = ((uint64_t)_src1->x[4]) * ((uint64_t)_src2->x[1]);
        const uint32_t a_4__b_1_lo = a_4__b_1;
        const uint32_t a_4__b_1_hi = a_4__b_1 >> 32;
        const uint64_t a_4__b_2 = ((uint64_t)_src1->x[4]) * ((uint64_t)_src2->x[2]);
        const uint32_t a_4__b_2_lo = a_4__b_2;
        const uint32_t a_4__b_2_hi = a_4__b_2 >> 32;
        const uint64_t a_4__b_3 = ((uint64_t)_src1->x[4]) * ((uint64_t)_src2->x[3]);
        const uint32_t a_4__b_3_lo = a_4__b_3;
        const uint32_t a_4__b_3_hi = a_4__b_3 >> 32;
        const uint64_t a_4__b_4 = ((uint64_t)_src1->x[4]) * ((uint64_t)_src2->x[4]);
        const uint32_t a_4__b_4_lo = a_4__b_4;
        const uint32_t a_4__b_4_hi = a_4__b_4 >> 32;
        
        /* Limbs of the product C = A*B */
        uint32_t c_0 = 0;
        uint32_t q_0 = 0;
        uint32_t c_1 = 0;
        uint32_t q_1 = 0;
        uint32_t c_2 = 0;
        uint32_t q_2 = 0;
        uint32_t c_3 = 0;
        uint32_t q_3 = 0;
        uint32_t c_4 = 0;
        uint32_t q_4 = 0;
        uint32_t c_5 = 0;
        uint32_t q_5 = 0;
        uint32_t c_6 = 0;
        uint32_t q_6 = 0;
        uint32_t c_7 = 0;
        uint32_t q_7 = 0;
        uint32_t c_8 = 0;
        uint32_t q_8 = 0;
        uint32_t c_9 = 0;
        uint32_t q_9 = 0;
        uint32_t c_10 = 0;
        uint32_t q_10 = 0;
        
        /* The product of the q_i's with N */
        uint64_t q_0__N_0 = 0;
        uint64_t q_0__N_1 = 0;
        uint64_t q_0__N_2 = 0;
        uint64_t q_0__N_3 = 0;
        uint64_t q_0__N_4 = 0;
        uint64_t q_1__N_0 = 0;
        uint64_t q_1__N_1 = 0;
        uint64_t q_1__N_2 = 0;
        uint64_t q_1__N_3 = 0;
        uint64_t q_1__N_4 = 0;
        uint64_t q_2__N_0 = 0;
        uint64_t q_2__N_1 = 0;
        uint64_t q_2__N_2 = 0;
        uint64_t q_2__N_3 = 0;
        uint64_t q_2__N_4 = 0;
        uint64_t q_3__N_0 = 0;
        uint64_t q_3__N_1 = 0;
        uint64_t q_3__N_2 = 0;
        uint64_t q_3__N_3 = 0;
        uint64_t q_3__N_4 = 0;
        uint64_t q_4__N_0 = 0;
        uint64_t q_4__N_1 = 0;
        uint64_t q_4__N_2 = 0;
        uint64_t q_4__N_3 = 0;
        uint64_t q_4__N_4 = 0;
        
        /* Compute c_0 */
        {
            /* Add the product terms into c_0, accumulating carries in c_1 */
            c_1 += ul32_addc(&c_0, &c_0, &a_0__b_0_lo);
            
            /* Add the q_i*N's for i < 0 */
            
            /* Compute q_0 and add its product with N */
            q_0 = n->np * c_0;
            q_0__N_0 = ((uint64_t)q_0) * ((uint64_t)n->n->x[0]);
            q_0__N_1 = ((uint64_t)q_0) * ((uint64_t)n->n->x[1]);
            q_0__N_2 = ((uint64_t)q_0) * ((uint64_t)n->n->x[2]);
            q_0__N_3 = ((uint64_t)q_0) * ((uint64_t)n->n->x[3]);
            q_0__N_4 = ((uint64_t)q_0) * ((uint64_t)n->n->x[4]);
            const uint32_t q_0__N_0_lo = q_0__N_0;
            c_1 += ul32_addc(&c_0, &c_0, &q_0__N_0_lo);
        }
        
        /* Compute c_1 */
        {
            /* Add the product terms into c_1, accumulating carries in c_2 */
            c_2 += ul32_addc(&c_1, &c_1, &a_0__b_0_hi);
            c_2 += ul32_addc(&c_1, &c_1, &a_0__b_1_lo);
            c_2 += ul32_addc(&c_1, &c_1, &a_1__b_0_lo);
            
            /* Add the q_i*N's for i < 1 */
            const uint32_t q_0__N_1_lo = q_0__N_1;
            c_2 += ul32_addc(&c_1, &c_1, &q_0__N_1_lo);
            const uint32_t q_0__N_0_hi = q_0__N_0 >> 32;
            c_2 += ul32_addc(&c_1, &c_1, &q_0__N_0_hi);
            
            /* Compute q_1 and add its product with N */
            q_1 = n->np * c_1;
            q_1__N_0 = ((uint64_t)q_1) * ((uint64_t)n->n->x[0]);
            q_1__N_1 = ((uint64_t)q_1) * ((uint64_t)n->n->x[1]);
            q_1__N_2 = ((uint64_t)q_1) * ((uint64_t)n->n->x[2]);
            q_1__N_3 = ((uint64_t)q_1) * ((uint64_t)n->n->x[3]);
            q_1__N_4 = ((uint64_t)q_1) * ((uint64_t)n->n->x[4]);
            const uint32_t q_1__N_0_lo = q_1__N_0;
            c_2 += ul32_addc(&c_1, &c_1, &q_1__N_0_lo);
        }
        
        /* Compute c_2 */
        {
            /* Add the product terms into c_2, accumulating carries in c_3 */
            c_3 += ul32_addc(&c_2, &c_2, &a_0__b_1_hi);
            c_3 += ul32_addc(&c_2, &c_2, &a_0__b_2_lo);
            c_3 += ul32_addc(&c_2, &c_2, &a_1__b_0_hi);
            c_3 += ul32_addc(&c_2, &c_2, &a_1__b_1_lo);
            c_3 += ul32_addc(&c_2, &c_2, &a_2__b_0_lo);
            
            /* Add the q_i*N's for i < 2 */
            const uint32_t q_0__N_2_lo = q_0__N_2;
            c_3 += ul32_addc(&c_2, &c_2, &q_0__N_2_lo);
            const uint32_t q_0__N_1_hi = q_0__N_1 >> 32;
            c_3 += ul32_addc(&c_2, &c_2, &q_0__N_1_hi);
            const uint32_t q_1__N_1_lo = q_1__N_1;
            c_3 += ul32_addc(&c_2, &c_2, &q_1__N_1_lo);
            const uint32_t q_1__N_0_hi = q_1__N_0 >> 32;
            c_3 += ul32_addc(&c_2, &c_2, &q_1__N_0_hi);
            
            /* Compute q_2 and add its product with N */
            q_2 = n->np * c_2;
            q_2__N_0 = ((uint64_t)q_2) * ((uint64_t)n->n->x[0]);
            q_2__N_1 = ((uint64_t)q_2) * ((uint64_t)n->n->x[1]);
            q_2__N_2 = ((uint64_t)q_2) * ((uint64_t)n->n->x[2]);
            q_2__N_3 = ((uint64_t)q_2) * ((uint64_t)n->n->x[3]);
            q_2__N_4 = ((uint64_t)q_2) * ((uint64_t)n->n->x[4]);
            const uint32_t q_2__N_0_lo = q_2__N_0;
            c_3 += ul32_addc(&c_2, &c_2, &q_2__N_0_lo);
        }
        
        /* Compute c_3 */
        {
            /* Add the product terms into c_3, accumulating carries in c_4 */
            c_4 += ul32_addc(&c_3, &c_3, &a_0__b_2_hi);
            c_4 += ul32_addc(&c_3, &c_3, &a_0__b_3_lo);
            c_4 += ul32_addc(&c_3, &c_3, &a_1__b_1_hi);
            c_4 += ul32_addc(&c_3, &c_3, &a_1__b_2_lo);
            c_4 += ul32_addc(&c_3, &c_3, &a_2__b_0_hi);
            c_4 += ul32_addc(&c_3, &c_3, &a_2__b_1_lo);
            c_4 += ul32_addc(&c_3, &c_3, &a_3__b_0_lo);
            
            /* Add the q_i*N's for i < 3 */
            const uint32_t q_0__N_3_lo = q_0__N_3;
            c_4 += ul32_addc(&c_3, &c_3, &q_0__N_3_lo);
            const uint32_t q_0__N_2_hi = q_0__N_2 >> 32;
            c_4 += ul32_addc(&c_3, &c_3, &q_0__N_2_hi);
            const uint32_t q_1__N_2_lo = q_1__N_2;
            c_4 += ul32_addc(&c_3, &c_3, &q_1__N_2_lo);
            const uint32_t q_1__N_1_hi = q_1__N_1 >> 32;
            c_4 += ul32_addc(&c_3, &c_3, &q_1__N_1_hi);
            const uint32_t q_2__N_1_lo = q_2__N_1;
            c_4 += ul32_addc(&c_3, &c_3, &q_2__N_1_lo);
            const uint32_t q_2__N_0_hi = q_2__N_0 >> 32;
            c_4 += ul32_addc(&c_3, &c_3, &q_2__N_0_hi);
            
            /* Compute q_3 and add its product with N */
            q_3 = n->np * c_3;
            q_3__N_0 = ((uint64_t)q_3) * ((uint64_t)n->n->x[0]);
            q_3__N_1 = ((uint64_t)q_3) * ((uint64_t)n->n->x[1]);
            q_3__N_2 = ((uint64_t)q_3) * ((uint64_t)n->n->x[2]);
            q_3__N_3 = ((uint64_t)q_3) * ((uint64_t)n->n->x[3]);
            q_3__N_4 = ((uint64_t)q_3) * ((uint64_t)n->n->x[4]);
            const uint32_t q_3__N_0_lo = q_3__N_0;
            c_4 += ul32_addc(&c_3, &c_3, &q_3__N_0_lo);
        }
        
        /* Compute c_4 */
        {
            /* Add the product terms into c_4, accumulating carries in c_5 */
            c_5 += ul32_addc(&c_4, &c_4, &a_0__b_3_hi);
            c_5 += ul32_addc(&c_4, &c_4, &a_0__b_4_lo);
            c_5 += ul32_addc(&c_4, &c_4, &a_1__b_2_hi);
            c_5 += ul32_addc(&c_4, &c_4, &a_1__b_3_lo);
            c_5 += ul32_addc(&c_4, &c_4, &a_2__b_1_hi);
            c_5 += ul32_addc(&c_4, &c_4, &a_2__b_2_lo);
            c_5 += ul32_addc(&c_4, &c_4, &a_3__b_0_hi);
            c_5 += ul32_addc(&c_4, &c_4, &a_3__b_1_lo);
            c_5 += ul32_addc(&c_4, &c_4, &a_4__b_0_lo);
            
            /* Add the q_i*N's for i < 4 */
            const uint32_t q_0__N_4_lo = q_0__N_4;
            c_5 += ul32_addc(&c_4, &c_4, &q_0__N_4_lo);
            const uint32_t q_0__N_3_hi = q_0__N_3 >> 32;
            c_5 += ul32_addc(&c_4, &c_4, &q_0__N_3_hi);
            const uint32_t q_1__N_3_lo = q_1__N_3;
            c_5 += ul32_addc(&c_4, &c_4, &q_1__N_3_lo);
            const uint32_t q_1__N_2_hi = q_1__N_2 >> 32;
            c_5 += ul32_addc(&c_4, &c_4, &q_1__N_2_hi);
            const uint32_t q_2__N_2_lo = q_2__N_2;
            c_5 += ul32_addc(&c_4, &c_4, &q_2__N_2_lo);
            const uint32_t q_2__N_1_hi = q_2__N_1 >> 32;
            c_5 += ul32_addc(&c_4, &c_4, &q_2__N_1_hi);
            const uint32_t q_3__N_1_lo = q_3__N_1;
            c_5 += ul32_addc(&c_4, &c_4, &q_3__N_1_lo);
            const uint32_t q_3__N_0_hi = q_3__N_0 >> 32;
            c_5 += ul32_addc(&c_4, &c_4, &q_3__N_0_hi);
            
            /* Compute q_4 and add its product with N */
            q_4 = n->np * c_4;
            q_4__N_0 = ((uint64_t)q_4) * ((uint64_t)n->n->x[0]);
            q_4__N_1 = ((uint64_t)q_4) * ((uint64_t)n->n->x[1]);
            q_4__N_2 = ((uint64_t)q_4) * ((uint64_t)n->n->x[2]);
            q_4__N_3 = ((uint64_t)q_4) * ((uint64_t)n->n->x[3]);
            q_4__N_4 = ((uint64_t)q_4) * ((uint64_t)n->n->x[4]);
            const uint32_t q_4__N_0_lo = q_4__N_0;
            c_5 += ul32_addc(&c_4, &c_4, &q_4__N_0_lo);
        }
        
        /* Compute c_5 */
        {
            /* Add the product terms into c_5, accumulating carries in c_6 */
            c_6 += ul32_addc(&c_5, &c_5, &a_0__b_4_hi);
            c_6 += ul32_addc(&c_5, &c_5, &a_1__b_3_hi);
            c_6 += ul32_addc(&c_5, &c_5, &a_1__b_4_lo);
            c_6 += ul32_addc(&c_5, &c_5, &a_2__b_2_hi);
            c_6 += ul32_addc(&c_5, &c_5, &a_2__b_3_lo);
            c_6 += ul32_addc(&c_5, &c_5, &a_3__b_1_hi);
            c_6 += ul32_addc(&c_5, &c_5, &a_3__b_2_lo);
            c_6 += ul32_addc(&c_5, &c_5, &a_4__b_0_hi);
            c_6 += ul32_addc(&c_5, &c_5, &a_4__b_1_lo);
            
            /* Add the q_i*N's for i < 5 */
            const uint32_t q_0__N_4_hi = q_0__N_4 >> 32;
            c_6 += ul32_addc(&c_5, &c_5, &q_0__N_4_hi);
            const uint32_t q_1__N_4_lo = q_1__N_4;
            c_6 += ul32_addc(&c_5, &c_5, &q_1__N_4_lo);
            const uint32_t q_1__N_3_hi = q_1__N_3 >> 32;
            c_6 += ul32_addc(&c_5, &c_5, &q_1__N_3_hi);
            const uint32_t q_2__N_3_lo = q_2__N_3;
            c_6 += ul32_addc(&c_5, &c_5, &q_2__N_3_lo);
            const uint32_t q_2__N_2_hi = q_2__N_2 >> 32;
            c_6 += ul32_addc(&c_5, &c_5, &q_2__N_2_hi);
            const uint32_t q_3__N_2_lo = q_3__N_2;
            c_6 += ul32_addc(&c_5, &c_5, &q_3__N_2_lo);
            const uint32_t q_3__N_1_hi = q_3__N_1 >> 32;
            c_6 += ul32_addc(&c_5, &c_5, &q_3__N_1_hi);
            const uint32_t q_4__N_1_lo = q_4__N_1;
            c_6 += ul32_addc(&c_5, &c_5, &q_4__N_1_lo);
            const uint32_t q_4__N_0_hi = q_4__N_0 >> 32;
            c_6 += ul32_addc(&c_5, &c_5, &q_4__N_0_hi);
            
        }
        
        /* Compute c_6 */
        {
            /* Add the product terms into c_6, accumulating carries in c_7 */
            c_7 += ul32_addc(&c_6, &c_6, &a_1__b_4_hi);
            c_7 += ul32_addc(&c_6, &c_6, &a_2__b_3_hi);
            c_7 += ul32_addc(&c_6, &c_6, &a_2__b_4_lo);
            c_7 += ul32_addc(&c_6, &c_6, &a_3__b_2_hi);
            c_7 += ul32_addc(&c_6, &c_6, &a_3__b_3_lo);
            c_7 += ul32_addc(&c_6, &c_6, &a_4__b_1_hi);
            c_7 += ul32_addc(&c_6, &c_6, &a_4__b_2_lo);
            
            /* Add the q_i*N's for i < 6 */
            const uint32_t q_1__N_4_hi = q_1__N_4 >> 32;
            c_7 += ul32_addc(&c_6, &c_6, &q_1__N_4_hi);
            const uint32_t q_2__N_4_lo = q_2__N_4;
            c_7 += ul32_addc(&c_6, &c_6, &q_2__N_4_lo);
            const uint32_t q_2__N_3_hi = q_2__N_3 >> 32;
            c_7 += ul32_addc(&c_6, &c_6, &q_2__N_3_hi);
            const uint32_t q_3__N_3_lo = q_3__N_3;
            c_7 += ul32_addc(&c_6, &c_6, &q_3__N_3_lo);
            const uint32_t q_3__N_2_hi = q_3__N_2 >> 32;
            c_7 += ul32_addc(&c_6, &c_6, &q_3__N_2_hi);
            const uint32_t q_4__N_2_lo = q_4__N_2;
            c_7 += ul32_addc(&c_6, &c_6, &q_4__N_2_lo);
            const uint32_t q_4__N_1_hi = q_4__N_1 >> 32;
            c_7 += ul32_addc(&c_6, &c_6, &q_4__N_1_hi);
            
        }
        
        /* Compute c_7 */
        {
            /* Add the product terms into c_7, accumulating carries in c_8 */
            c_8 += ul32_addc(&c_7, &c_7, &a_2__b_4_hi);
            c_8 += ul32_addc(&c_7, &c_7, &a_3__b_3_hi);
            c_8 += ul32_addc(&c_7, &c_7, &a_3__b_4_lo);
            c_8 += ul32_addc(&c_7, &c_7, &a_4__b_2_hi);
            c_8 += ul32_addc(&c_7, &c_7, &a_4__b_3_lo);
            
            /* Add the q_i*N's for i < 7 */
            const uint32_t q_2__N_4_hi = q_2__N_4 >> 32;
            c_8 += ul32_addc(&c_7, &c_7, &q_2__N_4_hi);
            const uint32_t q_3__N_4_lo = q_3__N_4;
            c_8 += ul32_addc(&c_7, &c_7, &q_3__N_4_lo);
            const uint32_t q_3__N_3_hi = q_3__N_3 >> 32;
            c_8 += ul32_addc(&c_7, &c_7, &q_3__N_3_hi);
            const uint32_t q_4__N_3_lo = q_4__N_3;
            c_8 += ul32_addc(&c_7, &c_7, &q_4__N_3_lo);
            const uint32_t q_4__N_2_hi = q_4__N_2 >> 32;
            c_8 += ul32_addc(&c_7, &c_7, &q_4__N_2_hi);
            
        }
        
        /* Compute c_8 */
        {
            /* Add the product terms into c_8, accumulating carries in c_9 */
            c_9 += ul32_addc(&c_8, &c_8, &a_3__b_4_hi);
            c_9 += ul32_addc(&c_8, &c_8, &a_4__b_3_hi);
            c_9 += ul32_addc(&c_8, &c_8, &a_4__b_4_lo);
            
            /* Add the q_i*N's for i < 8 */
            const uint32_t q_3__N_4_hi = q_3__N_4 >> 32;
            c_9 += ul32_addc(&c_8, &c_8, &q_3__N_4_hi);
            const uint32_t q_4__N_4_lo = q_4__N_4;
            c_9 += ul32_addc(&c_8, &c_8, &q_4__N_4_lo);
            const uint32_t q_4__N_3_hi = q_4__N_3 >> 32;
            c_9 += ul32_addc(&c_8, &c_8, &q_4__N_3_hi);
            
        }
        
        /* Compute c_9 */
        {
            /* Add the product terms into c_9, accumulating carries in c_10 */
            c_10 += ul32_addc(&c_9, &c_9, &a_4__b_4_hi);
            
            /* Add the q_i*N's for i < 9 */
            const uint32_t q_4__N_4_hi = q_4__N_4 >> 32;
            c_10 += ul32_addc(&c_9, &c_9, &q_4__N_4_hi);
            
        }
        
        /* R = C * beta^{-n} */
        _dst->x[0] = c_5;
        _dst->x[1] = c_6;
        _dst->x[2] = c_7;
        _dst->x[3] = c_8;
        _dst->x[4] = c_9;
        
        /* Reduce as needed */
        if (c_10 || (ul160_cmp(_dst, n->n) >= 0))
            ul160_sub(_dst, _dst, n->n);
    #endif
}

/*
 * Convert a ul160 into Montgomery form
 */
inline void ul160_to_montgomery(ul160 dst, ul160 src, mod160 mod) {
    ul160_modmul(dst, src, mod->rsq, mod);
}

/*
 * Convert a ul160 out-of Montgomery form
 */
inline void ul160_from_montgomery(ul160 dst, ul160 src, mod160 mod) {
    ul160 one = { 0 };
    one->x[0] = 1;
    
    ul160_modmul(dst, src, one, mod);
}




/*
 * Right-shift a ul160 by some number of bits
 */
inline void ul160_rshift(ul160 dst, ul160 src, int shift) {
dst->x[0] = (src->x[0] >> shift) | (src->x[1] << (32 - shift));
dst->x[1] = (src->x[1] >> shift) | (src->x[2] << (32 - shift));
dst->x[2] = (src->x[2] >> shift) | (src->x[3] << (32 - shift));
dst->x[3] = (src->x[3] >> shift) | (src->x[4] << (32 - shift));
dst->x[4] = dst->x[4] >> shift;
}



/*
 * Left shift a ul160 by some number of words
 */
inline void ul160_lshiftw(ul160 dst, ul160 src, int w) {
    dst->x[4] = ((4-w) >= 0) ? src->x[4-w] : 0;
    dst->x[3] = ((3-w) >= 0) ? src->x[3-w] : 0;
    dst->x[2] = ((2-w) >= 0) ? src->x[2-w] : 0;
    dst->x[1] = ((1-w) >= 0) ? src->x[1-w] : 0;
    dst->x[0] = ((0-w) >= 0) ? src->x[0-w] : 0;
}

/*
 * Multiply a ul160 by a uint32_t
 */
inline void ul160_mulu32(ul160 dst, ul160 src, uint32_t x) {
    uint64_t x_src_0 = ((uint64_t)src->x[0]) * ((uint64_t)x);
    uint64_t x_src_1 = ((uint64_t)src->x[1]) * ((uint64_t)x);
    uint64_t x_src_2 = ((uint64_t)src->x[2]) * ((uint64_t)x);
    uint64_t x_src_3 = ((uint64_t)src->x[3]) * ((uint64_t)x);
    uint64_t x_src_4 = ((uint64_t)src->x[4]) * ((uint64_t)x);
    
    dst->x[0] = 0;
    dst->x[1] = 0;
    dst->x[2] = 0;
    dst->x[3] = 0;
    dst->x[4] = 0;
    
    *(uint64_t*)(&dst->x[0]) += x_src_0;
    *(uint64_t*)(&dst->x[1]) += x_src_1;
    *(uint64_t*)(&dst->x[2]) += x_src_2;
    *(uint64_t*)(&dst->x[3]) += x_src_3;
    dst->x[4] += x_src_4;
}




#endif
